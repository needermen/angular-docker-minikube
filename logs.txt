
==> Audit <==
|------------|---------------------------|----------|-------------|---------|---------------------|---------------------|
|  Command   |           Args            | Profile  |    User     | Version |     Start Time      |      End Time       |
|------------|---------------------------|----------|-------------|---------|---------------------|---------------------|
| start      |                           | minikube | chikovanits | v1.35.0 | 25 Feb 25 16:37 +04 | 25 Feb 25 16:40 +04 |
| start      |                           | minikube | chikovanits | v1.35.0 | 25 Feb 25 16:51 +04 | 25 Feb 25 16:51 +04 |
| service    | hello-minikube            | minikube | chikovanits | v1.35.0 | 25 Feb 25 16:53 +04 |                     |
| dashboard  |                           | minikube | chikovanits | v1.35.0 | 25 Feb 25 16:53 +04 |                     |
| service    | hello-minikube            | minikube | chikovanits | v1.35.0 | 25 Feb 25 16:54 +04 |                     |
| service    | hello-minikube            | minikube | chikovanits | v1.35.0 | 25 Feb 25 16:56 +04 |                     |
| ssh        |                           | minikube | chikovanits | v1.35.0 | 25 Feb 25 17:00 +04 |                     |
| docker-env |                           | minikube | chikovanits | v1.35.0 | 26 Feb 25 15:27 +04 | 26 Feb 25 15:27 +04 |
| docker-env |                           | minikube | chikovanits | v1.35.0 | 26 Feb 25 15:27 +04 | 26 Feb 25 15:27 +04 |
| docker-env |                           | minikube | chikovanits | v1.35.0 | 26 Feb 25 15:36 +04 |                     |
| start      |                           | minikube | chikovanits | v1.35.0 | 26 Feb 25 15:53 +04 | 26 Feb 25 15:53 +04 |
| service    | angular-app-service       | minikube | chikovanits | v1.35.0 | 26 Feb 25 15:55 +04 |                     |
| service    | angular-app-service --url | minikube | chikovanits | v1.35.0 | 26 Feb 25 15:56 +04 |                     |
| service    | angular-app-service       | minikube | chikovanits | v1.35.0 | 26 Feb 25 15:56 +04 |                     |
| service    | angular-app-service       | minikube | chikovanits | v1.35.0 | 26 Feb 25 15:58 +04 |                     |
| service    | angular-app-service       | minikube | chikovanits | v1.35.0 | 26 Feb 25 16:35 +04 |                     |
| service    | angular-app-service       | minikube | chikovanits | v1.35.0 | 26 Feb 25 16:41 +04 |                     |
| service    | angular-app-service       | minikube | chikovanits | v1.35.0 | 26 Feb 25 16:42 +04 |                     |
|------------|---------------------------|----------|-------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/02/26 15:53:04
Running on machine: chikovanitss-MacBook-Pro
Binary: Built with gc go1.23.4 for darwin/arm64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0226 15:53:04.726625    7191 out.go:345] Setting OutFile to fd 1 ...
I0226 15:53:04.727262    7191 out.go:397] isatty.IsTerminal(1) = true
I0226 15:53:04.727264    7191 out.go:358] Setting ErrFile to fd 2...
I0226 15:53:04.727267    7191 out.go:397] isatty.IsTerminal(2) = true
I0226 15:53:04.727442    7191 root.go:338] Updating PATH: /Users/chikovanits/.minikube/bin
W0226 15:53:04.727696    7191 root.go:314] Error reading config file at /Users/chikovanits/.minikube/config/config.json: open /Users/chikovanits/.minikube/config/config.json: no such file or directory
I0226 15:53:04.728394    7191 out.go:352] Setting JSON to false
I0226 15:53:04.753646    7191 start.go:129] hostinfo: {"hostname":"chikovanitss-MacBook-Pro.local","uptime":1207,"bootTime":1740569577,"procs":682,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"15.0.1","kernelVersion":"24.0.0","kernelArch":"arm64","virtualizationSystem":"","virtualizationRole":"","hostId":"1605335b-4573-50fb-8b24-f6a21ec8f94b"}
W0226 15:53:04.753755    7191 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0226 15:53:04.760401    7191 out.go:177] üòÑ  minikube v1.35.0 on Darwin 15.0.1 (arm64)
I0226 15:53:04.770651    7191 notify.go:220] Checking for updates...
W0226 15:53:04.770711    7191 preload.go:293] Failed to list preload files: open /Users/chikovanits/.minikube/cache/preloaded-tarball: no such file or directory
I0226 15:53:04.775412    7191 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0226 15:53:04.775542    7191 driver.go:394] Setting default libvirt URI to qemu:///system
I0226 15:53:04.800248    7191 docker.go:123] docker version: linux-27.5.1:Docker Desktop 4.38.0 (181591)
I0226 15:53:04.800352    7191 cli_runner.go:164] Run: docker system info --format "{{json .}}"
W0226 15:53:04.901987    7191 notify.go:59] Error getting json from minikube version url: error with http GET for endpoint https://storage.googleapis.com/minikube/releases-v2.json: Get "https://storage.googleapis.com/minikube/releases-v2.json": read tcp 10.139.78.55:52800->216.58.214.155:443: read: connection reset by peer
I0226 15:53:05.157087    7191 info.go:266] docker info: {ID:189ac6c8-20ad-4527-83be-ebff36be59c4 Containers:5 ContainersRunning:1 ContainersPaused:0 ContainersStopped:4 Images:4 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:70 OomKillDisable:false NGoroutines:107 SystemTime:2025-02-26 11:53:05.147933338 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:18 KernelVersion:6.12.5-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:11 MemTotal:8217432064 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///Users/chikovanits/Library/Containers/com.docker.docker/Data/docker-cli.sock] ExperimentalBuild:false ServerVersion:27.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb Expected:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb} RuncCommit:{ID:v1.1.12-0-g51d5e946 Expected:v1.1.12-0-g51d5e946} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:/Users/chikovanits/.docker/cli-plugins/docker-ai SchemaVersion:0.1.0 ShortDescription:Ask Gordon - Docker Agent Vendor:Docker Inc. Version:v0.7.3] map[Name:buildx Path:/Users/chikovanits/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.20.1-desktop.2] map[Name:compose Path:/Users/chikovanits/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.32.4-desktop.1] map[Name:debug Path:/Users/chikovanits/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:/Users/chikovanits/.docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.4] map[Name:dev Path:/Users/chikovanits/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/Users/chikovanits/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:/Users/chikovanits/.docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:/Users/chikovanits/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:/Users/chikovanits/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/Users/chikovanits/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.16.1]] Warnings:<nil>}}
I0226 15:53:05.159436    7191 out.go:177] ‚ú®  Using the docker driver based on existing profile
I0226 15:53:05.163060    7191 start.go:297] selected driver: docker
I0226 15:53:05.163074    7191 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:7788 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0226 15:53:05.163114    7191 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0226 15:53:05.163233    7191 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0226 15:53:05.222260    7191 info.go:266] docker info: {ID:189ac6c8-20ad-4527-83be-ebff36be59c4 Containers:5 ContainersRunning:1 ContainersPaused:0 ContainersStopped:4 Images:4 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:70 OomKillDisable:false NGoroutines:107 SystemTime:2025-02-26 11:53:05.213880171 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:18 KernelVersion:6.12.5-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:11 MemTotal:8217432064 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///Users/chikovanits/Library/Containers/com.docker.docker/Data/docker-cli.sock] ExperimentalBuild:false ServerVersion:27.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb Expected:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb} RuncCommit:{ID:v1.1.12-0-g51d5e946 Expected:v1.1.12-0-g51d5e946} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:/Users/chikovanits/.docker/cli-plugins/docker-ai SchemaVersion:0.1.0 ShortDescription:Ask Gordon - Docker Agent Vendor:Docker Inc. Version:v0.7.3] map[Name:buildx Path:/Users/chikovanits/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.20.1-desktop.2] map[Name:compose Path:/Users/chikovanits/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.32.4-desktop.1] map[Name:debug Path:/Users/chikovanits/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:/Users/chikovanits/.docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.4] map[Name:dev Path:/Users/chikovanits/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/Users/chikovanits/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:/Users/chikovanits/.docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:/Users/chikovanits/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:/Users/chikovanits/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/Users/chikovanits/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.16.1]] Warnings:<nil>}}
I0226 15:53:05.222441    7191 cni.go:84] Creating CNI manager for ""
I0226 15:53:05.222496    7191 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0226 15:53:05.222544    7191 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:7788 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0226 15:53:05.226437    7191 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0226 15:53:05.230039    7191 cache.go:121] Beginning downloading kic base image for docker with docker
I0226 15:53:05.233038    7191 out.go:177] üöú  Pulling base image v0.0.46 ...
I0226 15:53:05.239111    7191 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0226 15:53:05.239123    7191 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0226 15:53:05.285749    7191 cache.go:150] Downloading gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 to local cache
I0226 15:53:05.286056    7191 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local cache directory
I0226 15:53:05.286096    7191 image.go:68] Found gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local cache directory, skipping pull
I0226 15:53:05.286110    7191 image.go:137] gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 exists in cache, skipping pull
I0226 15:53:05.286121    7191 cache.go:153] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 as a tarball
I0226 15:53:05.286123    7191 cache.go:163] Loading gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 from local cache
W0226 15:53:05.368353    7191 preload.go:108] https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.32.0/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-arm64.tar.lz4 fetch error: Head "https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.32.0/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-arm64.tar.lz4": read tcp 10.139.78.55:52801->216.58.214.155:443: read: connection reset by peer
I0226 15:53:05.368441    7191 profile.go:143] Saving config to /Users/chikovanits/.minikube/profiles/minikube/config.json ...
I0226 15:53:05.368594    7191 cache.go:107] acquiring lock: {Name:mk0af6c7840d5a04f0bde0b7028afe965383cd64 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0226 15:53:05.368605    7191 cache.go:107] acquiring lock: {Name:mk742a83bcdf34eff7d39010c0953d63d2a1b785 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0226 15:53:05.368648    7191 cache.go:107] acquiring lock: {Name:mk4981f9aac961278edca61a0897dda91a003acf Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0226 15:53:05.368661    7191 cache.go:107] acquiring lock: {Name:mk865a6ceeba893987d17c717a4919aeaf9e9f38 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0226 15:53:05.368669    7191 cache.go:107] acquiring lock: {Name:mk459290165c2c87c4b77af25113adbfdd6f704a Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0226 15:53:05.368665    7191 cache.go:107] acquiring lock: {Name:mkd349b4d092b0b1b13a3c7796cb301328f1e55c Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0226 15:53:05.368773    7191 cache.go:107] acquiring lock: {Name:mkf195f23761584967c8ac06b1aa16f88b6f9993 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0226 15:53:05.368791    7191 cache.go:107] acquiring lock: {Name:mkaf82f98a7cd3d345338aab7ef71e182cd7ce87 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0226 15:53:05.369138    7191 cache.go:115] /Users/chikovanits/.minikube/cache/images/arm64/registry.k8s.io/kube-controller-manager_v1.32.0 exists
I0226 15:53:05.369149    7191 cache.go:96] cache image "registry.k8s.io/kube-controller-manager:v1.32.0" -> "/Users/chikovanits/.minikube/cache/images/arm64/registry.k8s.io/kube-controller-manager_v1.32.0" took 510.75¬µs
I0226 15:53:05.369162    7191 cache.go:80] save to tar file registry.k8s.io/kube-controller-manager:v1.32.0 -> /Users/chikovanits/.minikube/cache/images/arm64/registry.k8s.io/kube-controller-manager_v1.32.0 succeeded
I0226 15:53:05.369182    7191 cache.go:115] /Users/chikovanits/.minikube/cache/images/arm64/registry.k8s.io/kube-scheduler_v1.32.0 exists
I0226 15:53:05.369208    7191 cache.go:96] cache image "registry.k8s.io/kube-scheduler:v1.32.0" -> "/Users/chikovanits/.minikube/cache/images/arm64/registry.k8s.io/kube-scheduler_v1.32.0" took 560.375¬µs
I0226 15:53:05.369213    7191 cache.go:80] save to tar file registry.k8s.io/kube-scheduler:v1.32.0 -> /Users/chikovanits/.minikube/cache/images/arm64/registry.k8s.io/kube-scheduler_v1.32.0 succeeded
I0226 15:53:05.369258    7191 cache.go:115] /Users/chikovanits/.minikube/cache/images/arm64/registry.k8s.io/coredns/coredns_v1.11.3 exists
I0226 15:53:05.369260    7191 cache.go:115] /Users/chikovanits/.minikube/cache/images/arm64/registry.k8s.io/kube-proxy_v1.32.0 exists
I0226 15:53:05.369268    7191 cache.go:96] cache image "registry.k8s.io/coredns/coredns:v1.11.3" -> "/Users/chikovanits/.minikube/cache/images/arm64/registry.k8s.io/coredns/coredns_v1.11.3" took 523.333¬µs
I0226 15:53:05.369274    7191 cache.go:80] save to tar file registry.k8s.io/coredns/coredns:v1.11.3 -> /Users/chikovanits/.minikube/cache/images/arm64/registry.k8s.io/coredns/coredns_v1.11.3 succeeded
I0226 15:53:05.369293    7191 cache.go:96] cache image "registry.k8s.io/kube-proxy:v1.32.0" -> "/Users/chikovanits/.minikube/cache/images/arm64/registry.k8s.io/kube-proxy_v1.32.0" took 665.25¬µs
I0226 15:53:05.369307    7191 cache.go:80] save to tar file registry.k8s.io/kube-proxy:v1.32.0 -> /Users/chikovanits/.minikube/cache/images/arm64/registry.k8s.io/kube-proxy_v1.32.0 succeeded
I0226 15:53:05.371101    7191 cache.go:115] /Users/chikovanits/.minikube/cache/images/arm64/registry.k8s.io/kube-apiserver_v1.32.0 exists
I0226 15:53:05.371103    7191 cache.go:115] /Users/chikovanits/.minikube/cache/images/arm64/registry.k8s.io/etcd_3.5.16-0 exists
I0226 15:53:05.371103    7191 cache.go:115] /Users/chikovanits/.minikube/cache/images/arm64/registry.k8s.io/pause_3.10 exists
I0226 15:53:05.371104    7191 cache.go:115] /Users/chikovanits/.minikube/cache/images/arm64/gcr.io/k8s-minikube/storage-provisioner_v5 exists
I0226 15:53:05.371113    7191 cache.go:96] cache image "registry.k8s.io/etcd:3.5.16-0" -> "/Users/chikovanits/.minikube/cache/images/arm64/registry.k8s.io/etcd_3.5.16-0" took 2.455666ms
I0226 15:53:05.371115    7191 cache.go:80] save to tar file registry.k8s.io/etcd:3.5.16-0 -> /Users/chikovanits/.minikube/cache/images/arm64/registry.k8s.io/etcd_3.5.16-0 succeeded
I0226 15:53:05.371114    7191 cache.go:96] cache image "gcr.io/k8s-minikube/storage-provisioner:v5" -> "/Users/chikovanits/.minikube/cache/images/arm64/gcr.io/k8s-minikube/storage-provisioner_v5" took 2.540792ms
I0226 15:53:05.371113    7191 cache.go:96] cache image "registry.k8s.io/kube-apiserver:v1.32.0" -> "/Users/chikovanits/.minikube/cache/images/arm64/registry.k8s.io/kube-apiserver_v1.32.0" took 2.543958ms
I0226 15:53:05.371113    7191 cache.go:96] cache image "registry.k8s.io/pause:3.10" -> "/Users/chikovanits/.minikube/cache/images/arm64/registry.k8s.io/pause_3.10" took 2.4ms
I0226 15:53:05.371119    7191 cache.go:80] save to tar file gcr.io/k8s-minikube/storage-provisioner:v5 -> /Users/chikovanits/.minikube/cache/images/arm64/gcr.io/k8s-minikube/storage-provisioner_v5 succeeded
I0226 15:53:05.371119    7191 cache.go:80] save to tar file registry.k8s.io/kube-apiserver:v1.32.0 -> /Users/chikovanits/.minikube/cache/images/arm64/registry.k8s.io/kube-apiserver_v1.32.0 succeeded
I0226 15:53:05.371119    7191 cache.go:80] save to tar file registry.k8s.io/pause:3.10 -> /Users/chikovanits/.minikube/cache/images/arm64/registry.k8s.io/pause_3.10 succeeded
I0226 15:53:05.371122    7191 cache.go:87] Successfully saved all images to host disk.
I0226 15:53:10.420565    7191 cache.go:165] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 from cached tarball
I0226 15:53:10.420618    7191 cache.go:227] Successfully downloaded all kic artifacts
I0226 15:53:10.420763    7191 start.go:360] acquireMachinesLock for minikube: {Name:mk721be4166ee1d83ba67e3ddbba9ef4eaeebfa9 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0226 15:53:10.421266    7191 start.go:364] duration metric: took 479.792¬µs to acquireMachinesLock for "minikube"
I0226 15:53:10.421307    7191 start.go:96] Skipping create...Using existing machine configuration
I0226 15:53:10.421340    7191 fix.go:54] fixHost starting: 
I0226 15:53:10.422036    7191 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0226 15:53:10.443331    7191 fix.go:112] recreateIfNeeded on minikube: state=Running err=<nil>
W0226 15:53:10.443359    7191 fix.go:138] unexpected machine state, will restart: <nil>
I0226 15:53:10.448885    7191 out.go:177] üèÉ  Updating the running docker "minikube" container ...
I0226 15:53:10.455896    7191 machine.go:93] provisionDockerMachine start ...
I0226 15:53:10.456061    7191 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0226 15:53:10.475953    7191 main.go:141] libmachine: Using SSH client type: native
I0226 15:53:10.476330    7191 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104cfc790] 0x104cfefd0 <nil>  [] 0s} 127.0.0.1 52167 <nil> <nil>}
I0226 15:53:10.476337    7191 main.go:141] libmachine: About to run SSH command:
hostname
I0226 15:53:10.579608    7191 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0226 15:53:10.579626    7191 ubuntu.go:169] provisioning hostname "minikube"
I0226 15:53:10.579729    7191 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0226 15:53:10.600012    7191 main.go:141] libmachine: Using SSH client type: native
I0226 15:53:10.600362    7191 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104cfc790] 0x104cfefd0 <nil>  [] 0s} 127.0.0.1 52167 <nil> <nil>}
I0226 15:53:10.600367    7191 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0226 15:53:10.722080    7191 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0226 15:53:10.722333    7191 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0226 15:53:10.755694    7191 main.go:141] libmachine: Using SSH client type: native
I0226 15:53:10.756038    7191 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104cfc790] 0x104cfefd0 <nil>  [] 0s} 127.0.0.1 52167 <nil> <nil>}
I0226 15:53:10.756050    7191 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0226 15:53:10.872831    7191 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0226 15:53:10.872869    7191 ubuntu.go:175] set auth options {CertDir:/Users/chikovanits/.minikube CaCertPath:/Users/chikovanits/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/chikovanits/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/chikovanits/.minikube/machines/server.pem ServerKeyPath:/Users/chikovanits/.minikube/machines/server-key.pem ClientKeyPath:/Users/chikovanits/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/chikovanits/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/chikovanits/.minikube}
I0226 15:53:10.872929    7191 ubuntu.go:177] setting up certificates
I0226 15:53:10.872976    7191 provision.go:84] configureAuth start
I0226 15:53:10.873250    7191 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0226 15:53:10.907390    7191 provision.go:143] copyHostCerts
I0226 15:53:10.913591    7191 exec_runner.go:144] found /Users/chikovanits/.minikube/ca.pem, removing ...
I0226 15:53:10.913603    7191 exec_runner.go:203] rm: /Users/chikovanits/.minikube/ca.pem
I0226 15:53:10.913983    7191 exec_runner.go:151] cp: /Users/chikovanits/.minikube/certs/ca.pem --> /Users/chikovanits/.minikube/ca.pem (1090 bytes)
I0226 15:53:10.919075    7191 exec_runner.go:144] found /Users/chikovanits/.minikube/cert.pem, removing ...
I0226 15:53:10.919078    7191 exec_runner.go:203] rm: /Users/chikovanits/.minikube/cert.pem
I0226 15:53:10.919388    7191 exec_runner.go:151] cp: /Users/chikovanits/.minikube/certs/cert.pem --> /Users/chikovanits/.minikube/cert.pem (1135 bytes)
I0226 15:53:10.924525    7191 exec_runner.go:144] found /Users/chikovanits/.minikube/key.pem, removing ...
I0226 15:53:10.924527    7191 exec_runner.go:203] rm: /Users/chikovanits/.minikube/key.pem
I0226 15:53:10.924701    7191 exec_runner.go:151] cp: /Users/chikovanits/.minikube/certs/key.pem --> /Users/chikovanits/.minikube/key.pem (1679 bytes)
I0226 15:53:10.924996    7191 provision.go:117] generating server cert: /Users/chikovanits/.minikube/machines/server.pem ca-key=/Users/chikovanits/.minikube/certs/ca.pem private-key=/Users/chikovanits/.minikube/certs/ca-key.pem org=chikovanits.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0226 15:53:11.091810    7191 provision.go:177] copyRemoteCerts
I0226 15:53:11.091875    7191 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0226 15:53:11.091906    7191 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0226 15:53:11.109396    7191 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52167 SSHKeyPath:/Users/chikovanits/.minikube/machines/minikube/id_rsa Username:docker}
I0226 15:53:11.207626    7191 ssh_runner.go:362] scp /Users/chikovanits/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1090 bytes)
I0226 15:53:11.227470    7191 ssh_runner.go:362] scp /Users/chikovanits/.minikube/machines/server.pem --> /etc/docker/server.pem (1192 bytes)
I0226 15:53:11.240233    7191 ssh_runner.go:362] scp /Users/chikovanits/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0226 15:53:11.250572    7191 provision.go:87] duration metric: took 377.584417ms to configureAuth
I0226 15:53:11.250583    7191 ubuntu.go:193] setting minikube options for container-runtime
I0226 15:53:11.251018    7191 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0226 15:53:11.251118    7191 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0226 15:53:11.271508    7191 main.go:141] libmachine: Using SSH client type: native
I0226 15:53:11.271814    7191 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104cfc790] 0x104cfefd0 <nil>  [] 0s} 127.0.0.1 52167 <nil> <nil>}
I0226 15:53:11.271819    7191 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0226 15:53:11.383659    7191 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0226 15:53:11.383677    7191 ubuntu.go:71] root file system type: overlay
I0226 15:53:11.383812    7191 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0226 15:53:11.384001    7191 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0226 15:53:11.412409    7191 main.go:141] libmachine: Using SSH client type: native
I0226 15:53:11.412846    7191 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104cfc790] 0x104cfefd0 <nil>  [] 0s} 127.0.0.1 52167 <nil> <nil>}
I0226 15:53:11.412891    7191 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0226 15:53:11.537293    7191 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0226 15:53:11.537518    7191 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0226 15:53:11.571176    7191 main.go:141] libmachine: Using SSH client type: native
I0226 15:53:11.571654    7191 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104cfc790] 0x104cfefd0 <nil>  [] 0s} 127.0.0.1 52167 <nil> <nil>}
I0226 15:53:11.571665    7191 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0226 15:53:11.691477    7191 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0226 15:53:11.691511    7191 machine.go:96] duration metric: took 1.2355945s to provisionDockerMachine
I0226 15:53:11.691555    7191 start.go:293] postStartSetup for "minikube" (driver="docker")
I0226 15:53:11.691581    7191 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0226 15:53:11.691865    7191 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0226 15:53:11.691952    7191 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0226 15:53:11.727327    7191 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52167 SSHKeyPath:/Users/chikovanits/.minikube/machines/minikube/id_rsa Username:docker}
I0226 15:53:11.818326    7191 ssh_runner.go:195] Run: cat /etc/os-release
I0226 15:53:11.821765    7191 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0226 15:53:11.821853    7191 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0226 15:53:11.821877    7191 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0226 15:53:11.821914    7191 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0226 15:53:11.821934    7191 filesync.go:126] Scanning /Users/chikovanits/.minikube/addons for local assets ...
I0226 15:53:11.822895    7191 filesync.go:126] Scanning /Users/chikovanits/.minikube/files for local assets ...
I0226 15:53:11.823458    7191 start.go:296] duration metric: took 131.889667ms for postStartSetup
I0226 15:53:11.823619    7191 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0226 15:53:11.823747    7191 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0226 15:53:11.857954    7191 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52167 SSHKeyPath:/Users/chikovanits/.minikube/machines/minikube/id_rsa Username:docker}
I0226 15:53:11.946854    7191 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0226 15:53:11.950369    7191 fix.go:56] duration metric: took 1.529025666s for fixHost
I0226 15:53:11.950388    7191 start.go:83] releasing machines lock for "minikube", held for 1.529100375s
I0226 15:53:11.950586    7191 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0226 15:53:11.984643    7191 ssh_runner.go:195] Run: cat /version.json
I0226 15:53:11.984729    7191 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0226 15:53:11.984997    7191 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0226 15:53:11.985094    7191 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0226 15:53:12.008171    7191 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52167 SSHKeyPath:/Users/chikovanits/.minikube/machines/minikube/id_rsa Username:docker}
I0226 15:53:12.008165    7191 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52167 SSHKeyPath:/Users/chikovanits/.minikube/machines/minikube/id_rsa Username:docker}
I0226 15:53:12.088521    7191 ssh_runner.go:195] Run: systemctl --version
W0226 15:53:12.619575    7191 start.go:867] [curl -sS -m 2 https://registry.k8s.io/] failed: curl -sS -m 2 https://registry.k8s.io/: Process exited with status 35
stdout:

stderr:
curl: (35) error:0A000126:SSL routines::unexpected eof while reading
I0226 15:53:12.619666    7191 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0226 15:53:12.622185    7191 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0226 15:53:12.631031    7191 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0226 15:53:12.631098    7191 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0226 15:53:12.634575    7191 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0226 15:53:12.634588    7191 start.go:495] detecting cgroup driver to use...
I0226 15:53:12.634597    7191 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0226 15:53:12.634745    7191 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0226 15:53:12.643527    7191 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0226 15:53:12.647196    7191 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0226 15:53:12.651550    7191 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0226 15:53:12.651589    7191 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0226 15:53:12.655674    7191 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0226 15:53:12.659689    7191 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0226 15:53:12.663675    7191 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0226 15:53:12.667645    7191 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0226 15:53:12.672076    7191 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0226 15:53:12.675700    7191 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0226 15:53:12.679283    7191 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0226 15:53:12.682710    7191 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0226 15:53:12.686165    7191 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0226 15:53:12.688958    7191 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0226 15:53:12.714952    7191 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0226 15:53:12.772133    7191 start.go:495] detecting cgroup driver to use...
I0226 15:53:12.772148    7191 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0226 15:53:12.772232    7191 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0226 15:53:12.776833    7191 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0226 15:53:12.776888    7191 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0226 15:53:12.782576    7191 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0226 15:53:12.788273    7191 ssh_runner.go:195] Run: which cri-dockerd
I0226 15:53:12.789719    7191 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0226 15:53:12.792779    7191 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0226 15:53:12.798962    7191 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0226 15:53:12.828885    7191 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0226 15:53:12.855699    7191 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0226 15:53:12.855923    7191 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
W0226 15:53:12.855990    7191 out.go:270] ‚ùó  Failing to connect to https://registry.k8s.io/ from inside the minikube container
W0226 15:53:12.856048    7191 out.go:270] üí°  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0226 15:53:12.862145    7191 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0226 15:53:12.886528    7191 ssh_runner.go:195] Run: sudo systemctl restart docker
I0226 15:53:14.660093    7191 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.7735145s)
I0226 15:53:14.660360    7191 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0226 15:53:14.666929    7191 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0226 15:53:14.673188    7191 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0226 15:53:14.679525    7191 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0226 15:53:14.710813    7191 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0226 15:53:14.737133    7191 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0226 15:53:14.762649    7191 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0226 15:53:14.785637    7191 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0226 15:53:14.790286    7191 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0226 15:53:14.816198    7191 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0226 15:53:14.950721    7191 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0226 15:53:14.951127    7191 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0226 15:53:14.952913    7191 start.go:563] Will wait 60s for crictl version
I0226 15:53:14.953085    7191 ssh_runner.go:195] Run: which crictl
I0226 15:53:14.954767    7191 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0226 15:53:15.011747    7191 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I0226 15:53:15.011898    7191 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0226 15:53:15.056894    7191 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0226 15:53:15.072498    7191 out.go:235] üê≥  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
I0226 15:53:15.072791    7191 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0226 15:53:15.173684    7191 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0226 15:53:15.173885    7191 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0226 15:53:15.175696    7191 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0226 15:53:15.179704    7191 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0226 15:53:15.205418    7191 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:7788 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0226 15:53:15.205473    7191 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0226 15:53:15.205520    7191 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0226 15:53:15.214365    7191 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0226 15:53:15.214373    7191 cache_images.go:84] Images are preloaded, skipping loading
I0226 15:53:15.214381    7191 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...
I0226 15:53:15.214483    7191 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0226 15:53:15.214567    7191 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0226 15:53:15.306106    7191 cni.go:84] Creating CNI manager for ""
I0226 15:53:15.306122    7191 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0226 15:53:15.306147    7191 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0226 15:53:15.306166    7191 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0226 15:53:15.306308    7191 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0226 15:53:15.306443    7191 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0226 15:53:15.310608    7191 binaries.go:44] Found k8s binaries, skipping transfer
I0226 15:53:15.310778    7191 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0226 15:53:15.314000    7191 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0226 15:53:15.319875    7191 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0226 15:53:15.325316    7191 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0226 15:53:15.331057    7191 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0226 15:53:15.332635    7191 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0226 15:53:15.336047    7191 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0226 15:53:15.361264    7191 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0226 15:53:15.391586    7191 certs.go:68] Setting up /Users/chikovanits/.minikube/profiles/minikube for IP: 192.168.49.2
I0226 15:53:15.391598    7191 certs.go:194] generating shared ca certs ...
I0226 15:53:15.391635    7191 certs.go:226] acquiring lock for ca certs: {Name:mk5909cd9c0c52facd40c6e756d76056a3872853 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0226 15:53:15.396082    7191 certs.go:235] skipping valid "minikubeCA" ca cert: /Users/chikovanits/.minikube/ca.key
I0226 15:53:15.403136    7191 certs.go:235] skipping valid "proxyClientCA" ca cert: /Users/chikovanits/.minikube/proxy-client-ca.key
I0226 15:53:15.403172    7191 certs.go:256] generating profile certs ...
I0226 15:53:15.403793    7191 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /Users/chikovanits/.minikube/profiles/minikube/client.key
I0226 15:53:15.410006    7191 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /Users/chikovanits/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0226 15:53:15.414979    7191 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /Users/chikovanits/.minikube/profiles/minikube/proxy-client.key
I0226 15:53:15.419942    7191 certs.go:484] found cert: /Users/chikovanits/.minikube/certs/ca-key.pem (1679 bytes)
I0226 15:53:15.420091    7191 certs.go:484] found cert: /Users/chikovanits/.minikube/certs/ca.pem (1090 bytes)
I0226 15:53:15.420223    7191 certs.go:484] found cert: /Users/chikovanits/.minikube/certs/cert.pem (1135 bytes)
I0226 15:53:15.420340    7191 certs.go:484] found cert: /Users/chikovanits/.minikube/certs/key.pem (1679 bytes)
I0226 15:53:15.421203    7191 ssh_runner.go:362] scp /Users/chikovanits/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0226 15:53:15.429711    7191 ssh_runner.go:362] scp /Users/chikovanits/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0226 15:53:15.438622    7191 ssh_runner.go:362] scp /Users/chikovanits/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0226 15:53:15.446933    7191 ssh_runner.go:362] scp /Users/chikovanits/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0226 15:53:15.456235    7191 ssh_runner.go:362] scp /Users/chikovanits/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0226 15:53:15.464225    7191 ssh_runner.go:362] scp /Users/chikovanits/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0226 15:53:15.472627    7191 ssh_runner.go:362] scp /Users/chikovanits/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0226 15:53:15.480770    7191 ssh_runner.go:362] scp /Users/chikovanits/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0226 15:53:15.488936    7191 ssh_runner.go:362] scp /Users/chikovanits/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0226 15:53:15.497358    7191 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0226 15:53:15.504718    7191 ssh_runner.go:195] Run: openssl version
I0226 15:53:15.508660    7191 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0226 15:53:15.512892    7191 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0226 15:53:15.514423    7191 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Feb 25 12:40 /usr/share/ca-certificates/minikubeCA.pem
I0226 15:53:15.514524    7191 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0226 15:53:15.517460    7191 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0226 15:53:15.520890    7191 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0226 15:53:15.522296    7191 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0226 15:53:15.525434    7191 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0226 15:53:15.528300    7191 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0226 15:53:15.531135    7191 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0226 15:53:15.534476    7191 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0226 15:53:15.538326    7191 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0226 15:53:15.542157    7191 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:7788 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0226 15:53:15.542296    7191 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0226 15:53:15.550419    7191 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0226 15:53:15.553879    7191 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0226 15:53:15.553887    7191 kubeadm.go:593] restartPrimaryControlPlane start ...
I0226 15:53:15.553937    7191 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0226 15:53:15.557581    7191 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0226 15:53:15.557675    7191 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0226 15:53:15.579199    7191 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:64372"
I0226 15:53:15.579223    7191 kubeconfig.go:47] verify endpoint returned: got: 127.0.0.1:64372, want: 127.0.0.1:52171
I0226 15:53:15.579400    7191 kubeconfig.go:62] /Users/chikovanits/.kube/config needs updating (will repair): [kubeconfig needs server address update]
I0226 15:53:15.579529    7191 lock.go:35] WriteFile acquiring /Users/chikovanits/.kube/config: {Name:mk587e1c4efea00cc6526666f25f59a3dea66724 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0226 15:53:15.581802    7191 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0226 15:53:15.585527    7191 kubeadm.go:630] The running cluster does not require reconfiguration: 127.0.0.1
I0226 15:53:15.585541    7191 kubeadm.go:597] duration metric: took 31.651708ms to restartPrimaryControlPlane
I0226 15:53:15.585544    7191 kubeadm.go:394] duration metric: took 43.396834ms to StartCluster
I0226 15:53:15.585553    7191 settings.go:142] acquiring lock: {Name:mk727452de8f439eccfe6aad725c27cd58b954fe Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0226 15:53:15.585802    7191 settings.go:150] Updating kubeconfig:  /Users/chikovanits/.kube/config
I0226 15:53:15.586160    7191 lock.go:35] WriteFile acquiring /Users/chikovanits/.kube/config: {Name:mk587e1c4efea00cc6526666f25f59a3dea66724 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0226 15:53:15.586426    7191 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0226 15:53:15.586450    7191 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0226 15:53:15.586536    7191 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0226 15:53:15.586543    7191 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W0226 15:53:15.586544    7191 addons.go:247] addon storage-provisioner should already be in state true
I0226 15:53:15.586547    7191 addons.go:69] Setting dashboard=true in profile "minikube"
I0226 15:53:15.586558    7191 host.go:66] Checking if "minikube" exists ...
I0226 15:53:15.586559    7191 addons.go:238] Setting addon dashboard=true in "minikube"
W0226 15:53:15.586563    7191 addons.go:247] addon dashboard should already be in state true
I0226 15:53:15.586580    7191 host.go:66] Checking if "minikube" exists ...
I0226 15:53:15.586581    7191 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0226 15:53:15.586604    7191 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0226 15:53:15.586626    7191 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0226 15:53:15.586893    7191 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0226 15:53:15.586933    7191 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0226 15:53:15.587092    7191 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0226 15:53:15.590834    7191 out.go:177] üîé  Verifying Kubernetes components...
I0226 15:53:15.593870    7191 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0226 15:53:15.606757    7191 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0226 15:53:15.612801    7191 out.go:177]     ‚ñ™ Using image docker.io/kubernetesui/dashboard:v2.7.0
I0226 15:53:15.616852    7191 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0226 15:53:15.616857    7191 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0226 15:53:15.616923    7191 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0226 15:53:15.623745    7191 out.go:177]     ‚ñ™ Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I0226 15:53:15.624283    7191 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0226 15:53:15.625715    7191 addons.go:435] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0226 15:53:15.625719    7191 ssh_runner.go:362] scp dashboard/dashboard-ns.yaml --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0226 15:53:15.625770    7191 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0226 15:53:15.630053    7191 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0226 15:53:15.630712    7191 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52167 SSHKeyPath:/Users/chikovanits/.minikube/machines/minikube/id_rsa Username:docker}
I0226 15:53:15.638152    7191 addons.go:238] Setting addon default-storageclass=true in "minikube"
W0226 15:53:15.638159    7191 addons.go:247] addon default-storageclass should already be in state true
I0226 15:53:15.638172    7191 host.go:66] Checking if "minikube" exists ...
I0226 15:53:15.638560    7191 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0226 15:53:15.640273    7191 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52167 SSHKeyPath:/Users/chikovanits/.minikube/machines/minikube/id_rsa Username:docker}
I0226 15:53:15.644689    7191 api_server.go:52] waiting for apiserver process to appear ...
I0226 15:53:15.644785    7191 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0226 15:53:15.654166    7191 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0226 15:53:15.654173    7191 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0226 15:53:15.654240    7191 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0226 15:53:15.667167    7191 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52167 SSHKeyPath:/Users/chikovanits/.minikube/machines/minikube/id_rsa Username:docker}
I0226 15:53:15.717494    7191 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0226 15:53:15.718031    7191 addons.go:435] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0226 15:53:15.718034    7191 ssh_runner.go:362] scp dashboard/dashboard-clusterrole.yaml --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0226 15:53:15.724406    7191 addons.go:435] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0226 15:53:15.724412    7191 ssh_runner.go:362] scp dashboard/dashboard-clusterrolebinding.yaml --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0226 15:53:15.730845    7191 addons.go:435] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0226 15:53:15.730849    7191 ssh_runner.go:362] scp dashboard/dashboard-configmap.yaml --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0226 15:53:15.737513    7191 addons.go:435] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0226 15:53:15.737519    7191 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I0226 15:53:15.745278    7191 addons.go:435] installing /etc/kubernetes/addons/dashboard-role.yaml
I0226 15:53:15.745282    7191 ssh_runner.go:362] scp dashboard/dashboard-role.yaml --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0226 15:53:15.751884    7191 addons.go:435] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0226 15:53:15.751888    7191 ssh_runner.go:362] scp dashboard/dashboard-rolebinding.yaml --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I0226 15:53:15.758416    7191 addons.go:435] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0226 15:53:15.758423    7191 ssh_runner.go:362] scp dashboard/dashboard-sa.yaml --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I0226 15:53:15.764262    7191 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0226 15:53:15.764955    7191 addons.go:435] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0226 15:53:15.764958    7191 ssh_runner.go:362] scp dashboard/dashboard-secret.yaml --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I0226 15:53:15.772313    7191 addons.go:435] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0226 15:53:15.772321    7191 ssh_runner.go:362] scp dashboard/dashboard-svc.yaml --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I0226 15:53:15.779402    7191 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
W0226 15:53:15.814516    7191 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0226 15:53:15.814548    7191 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0226 15:53:15.814555    7191 retry.go:31] will retry after 368.67313ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0226 15:53:15.814557    7191 retry.go:31] will retry after 231.42049ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0226 15:53:15.815193    7191 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0226 15:53:15.815199    7191 retry.go:31] will retry after 203.849311ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0226 15:53:16.020337    7191 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0226 15:53:16.047173    7191 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0226 15:53:16.051189    7191 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0226 15:53:16.051221    7191 retry.go:31] will retry after 388.2434ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0226 15:53:16.075824    7191 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0226 15:53:16.075844    7191 retry.go:31] will retry after 479.877649ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0226 15:53:16.146071    7191 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0226 15:53:16.184226    7191 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0226 15:53:16.211651    7191 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0226 15:53:16.211685    7191 retry.go:31] will retry after 394.319838ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0226 15:53:16.440505    7191 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
W0226 15:53:16.468160    7191 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0226 15:53:16.468178    7191 retry.go:31] will retry after 394.486355ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0226 15:53:16.557091    7191 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0226 15:53:16.595392    7191 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0226 15:53:16.595411    7191 retry.go:31] will retry after 685.040294ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0226 15:53:16.607188    7191 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0226 15:53:16.633483    7191 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0226 15:53:16.633499    7191 retry.go:31] will retry after 429.281935ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0226 15:53:16.646191    7191 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0226 15:53:16.863679    7191 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
W0226 15:53:16.898573    7191 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0226 15:53:16.898604    7191 retry.go:31] will retry after 1.000153389s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0226 15:53:17.064182    7191 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0226 15:53:17.145672    7191 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0226 15:53:17.282030    7191 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0226 15:53:17.899763    7191 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0226 15:53:18.044256    7191 api_server.go:72] duration metric: took 2.457794292s to wait for apiserver process to appear ...
I0226 15:53:18.044277    7191 api_server.go:88] waiting for apiserver healthz status ...
I0226 15:53:18.044324    7191 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:52171/healthz ...
I0226 15:53:18.054892    7191 api_server.go:279] https://127.0.0.1:52171/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0226 15:53:18.054902    7191 api_server.go:103] status: https://127.0.0.1:52171/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0226 15:53:18.446812    7191 out.go:177] üí°  Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server

I0226 15:53:18.452096    7191 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass, dashboard
I0226 15:53:18.463845    7191 addons.go:514] duration metric: took 2.877371208s for enable addons: enabled=[storage-provisioner default-storageclass dashboard]
I0226 15:53:18.545245    7191 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:52171/healthz ...
I0226 15:53:18.552066    7191 api_server.go:279] https://127.0.0.1:52171/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0226 15:53:18.552076    7191 api_server.go:103] status: https://127.0.0.1:52171/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0226 15:53:19.045487    7191 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:52171/healthz ...
I0226 15:53:19.052796    7191 api_server.go:279] https://127.0.0.1:52171/healthz returned 200:
ok
I0226 15:53:19.054042    7191 api_server.go:141] control plane version: v1.32.0
I0226 15:53:19.054065    7191 api_server.go:131] duration metric: took 1.00977025s to wait for apiserver health ...
I0226 15:53:19.054090    7191 system_pods.go:43] waiting for kube-system pods to appear ...
I0226 15:53:19.062121    7191 system_pods.go:59] 7 kube-system pods found
I0226 15:53:19.062136    7191 system_pods.go:61] "coredns-668d6bf9bc-84752" [f6ed9759-917f-4e1d-824f-f61ef4e38a61] Running
I0226 15:53:19.062140    7191 system_pods.go:61] "etcd-minikube" [cee6b367-989c-48d8-adb9-a65818130be3] Running
I0226 15:53:19.062143    7191 system_pods.go:61] "kube-apiserver-minikube" [0d2209af-b943-4ed5-a2d7-b43b02d1baf7] Running
I0226 15:53:19.062145    7191 system_pods.go:61] "kube-controller-manager-minikube" [43ccb5b9-b687-4916-91ec-0341af54b6e3] Running
I0226 15:53:19.062147    7191 system_pods.go:61] "kube-proxy-gdwpw" [d00c55e9-bd32-4f39-9edc-f93110f3c6b5] Running
I0226 15:53:19.062149    7191 system_pods.go:61] "kube-scheduler-minikube" [2072c1bc-d5f3-4961-aa9f-8d2ffd3abd43] Running
I0226 15:53:19.062151    7191 system_pods.go:61] "storage-provisioner" [d02a0cdd-b4ab-4c93-8170-ef8e5701462f] Running
I0226 15:53:19.062155    7191 system_pods.go:74] duration metric: took 8.060375ms to wait for pod list to return data ...
I0226 15:53:19.062163    7191 kubeadm.go:582] duration metric: took 3.475704417s to wait for: map[apiserver:true system_pods:true]
I0226 15:53:19.062176    7191 node_conditions.go:102] verifying NodePressure condition ...
I0226 15:53:19.064189    7191 node_conditions.go:122] node storage ephemeral capacity is 1055761844Ki
I0226 15:53:19.064196    7191 node_conditions.go:123] node cpu capacity is 11
I0226 15:53:19.064206    7191 node_conditions.go:105] duration metric: took 2.02575ms to run NodePressure ...
I0226 15:53:19.064212    7191 start.go:241] waiting for startup goroutines ...
I0226 15:53:19.064218    7191 start.go:246] waiting for cluster config update ...
I0226 15:53:19.064225    7191 start.go:255] writing updated cluster config ...
I0226 15:53:19.065322    7191 ssh_runner.go:195] Run: rm -f paused
I0226 15:53:19.121034    7191 start.go:600] kubectl: 1.32.2, cluster: 1.32.0 (minor skew: 0)
I0226 15:53:19.130770    7191 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Feb 26 12:19:46 minikube dockerd[1005]: time="2025-02-26T12:19:46.041373968Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:24:24 minikube dockerd[1005]: time="2025-02-26T12:24:24.060351597Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:24:24 minikube dockerd[1005]: time="2025-02-26T12:24:24.060447305Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:24:24 minikube dockerd[1005]: time="2025-02-26T12:24:24.063591013Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:24:26 minikube dockerd[1005]: time="2025-02-26T12:24:26.048321292Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:24:26 minikube dockerd[1005]: time="2025-02-26T12:24:26.048670667Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:24:26 minikube dockerd[1005]: time="2025-02-26T12:24:26.051877167Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:24:38 minikube dockerd[1005]: time="2025-02-26T12:24:38.034598006Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:24:38 minikube dockerd[1005]: time="2025-02-26T12:24:38.034727756Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:24:38 minikube dockerd[1005]: time="2025-02-26T12:24:38.036902797Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:24:52 minikube dockerd[1005]: time="2025-02-26T12:24:52.039124512Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:24:52 minikube dockerd[1005]: time="2025-02-26T12:24:52.039193887Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:24:52 minikube dockerd[1005]: time="2025-02-26T12:24:52.042762512Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:24:52 minikube dockerd[1005]: time="2025-02-26T12:24:52.518757804Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:24:52 minikube dockerd[1005]: time="2025-02-26T12:24:52.518832596Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:24:52 minikube dockerd[1005]: time="2025-02-26T12:24:52.521688554Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:29:32 minikube dockerd[1005]: time="2025-02-26T12:29:32.061645420Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:29:32 minikube dockerd[1005]: time="2025-02-26T12:29:32.061710711Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:29:32 minikube dockerd[1005]: time="2025-02-26T12:29:32.064448253Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:29:33 minikube dockerd[1005]: time="2025-02-26T12:29:33.040995920Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:29:33 minikube dockerd[1005]: time="2025-02-26T12:29:33.041068920Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:29:33 minikube dockerd[1005]: time="2025-02-26T12:29:33.043810795Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:29:52 minikube dockerd[1005]: time="2025-02-26T12:29:52.060734846Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:29:52 minikube dockerd[1005]: time="2025-02-26T12:29:52.060829262Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:29:52 minikube dockerd[1005]: time="2025-02-26T12:29:52.063215096Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:29:56 minikube dockerd[1005]: time="2025-02-26T12:29:56.084406417Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:29:56 minikube dockerd[1005]: time="2025-02-26T12:29:56.084497792Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:29:56 minikube dockerd[1005]: time="2025-02-26T12:29:56.087230667Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:29:58 minikube dockerd[1005]: time="2025-02-26T12:29:58.044695543Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:29:58 minikube dockerd[1005]: time="2025-02-26T12:29:58.044832376Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:29:58 minikube dockerd[1005]: time="2025-02-26T12:29:58.046882751Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:34:41 minikube dockerd[1005]: time="2025-02-26T12:34:41.096167091Z" level=info msg="Attempting next endpoint for pull after error: Head \"https://registry-1.docker.io/v2/kicbase/echo-server/manifests/1.0\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:34:41 minikube dockerd[1005]: time="2025-02-26T12:34:41.099449132Z" level=error msg="Handler for POST /v1.43/images/create returned error: Head \"https://registry-1.docker.io/v2/kicbase/echo-server/manifests/1.0\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:34:43 minikube dockerd[1005]: time="2025-02-26T12:34:43.032702133Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:34:43 minikube dockerd[1005]: time="2025-02-26T12:34:43.032740175Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:34:43 minikube dockerd[1005]: time="2025-02-26T12:34:43.035536966Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:35:00 minikube dockerd[1005]: time="2025-02-26T12:35:00.046474169Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:35:00 minikube dockerd[1005]: time="2025-02-26T12:35:00.046622502Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:35:00 minikube dockerd[1005]: time="2025-02-26T12:35:00.049079710Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:35:01 minikube dockerd[1005]: time="2025-02-26T12:35:01.031030044Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:35:01 minikube dockerd[1005]: time="2025-02-26T12:35:01.031110752Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:35:01 minikube dockerd[1005]: time="2025-02-26T12:35:01.034578544Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:35:08 minikube dockerd[1005]: time="2025-02-26T12:35:08.042478922Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:35:08 minikube dockerd[1005]: time="2025-02-26T12:35:08.042527964Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:35:08 minikube dockerd[1005]: time="2025-02-26T12:35:08.045049381Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:39:47 minikube dockerd[1005]: time="2025-02-26T12:39:47.063102802Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:39:47 minikube dockerd[1005]: time="2025-02-26T12:39:47.063584052Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:39:47 minikube dockerd[1005]: time="2025-02-26T12:39:47.066383427Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:39:47 minikube dockerd[1005]: time="2025-02-26T12:39:47.542924177Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:39:47 minikube dockerd[1005]: time="2025-02-26T12:39:47.542999469Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:39:47 minikube dockerd[1005]: time="2025-02-26T12:39:47.545604969Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:40:08 minikube dockerd[1005]: time="2025-02-26T12:40:08.052919214Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:40:08 minikube dockerd[1005]: time="2025-02-26T12:40:08.053007547Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:40:08 minikube dockerd[1005]: time="2025-02-26T12:40:08.055550881Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:40:10 minikube dockerd[1005]: time="2025-02-26T12:40:10.047912632Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:40:10 minikube dockerd[1005]: time="2025-02-26T12:40:10.047987007Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:40:10 minikube dockerd[1005]: time="2025-02-26T12:40:10.051148423Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:40:17 minikube dockerd[1005]: time="2025-02-26T12:40:17.045444093Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:40:17 minikube dockerd[1005]: time="2025-02-26T12:40:17.045642385Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Feb 26 12:40:17 minikube dockerd[1005]: time="2025-02-26T12:40:17.048182135Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
91ed578536abc       66749159455b3       48 minutes ago      Running             storage-provisioner       5                   60cb701a88f6d       storage-provisioner
c0b63bd99392d       2f6c962e7b831       49 minutes ago      Running             coredns                   2                   d705bb423ce9f       coredns-668d6bf9bc-84752
b9315df8de0e6       66749159455b3       49 minutes ago      Exited              storage-provisioner       4                   60cb701a88f6d       storage-provisioner
df55624022307       2f50386e20bfd       49 minutes ago      Running             kube-proxy                2                   9dc9bdcc92d41       kube-proxy-gdwpw
73b77df227349       2b5bd0f16085a       49 minutes ago      Running             kube-apiserver            2                   d177641afe3e0       kube-apiserver-minikube
e51553cf8a62f       a8d049396f6b8       49 minutes ago      Running             kube-controller-manager   2                   d4971d9325ad0       kube-controller-manager-minikube
3e99af83255e0       c3ff26fb59f37       49 minutes ago      Running             kube-scheduler            2                   9aa615ca0c367       kube-scheduler-minikube
8581616c15f95       7fc9d4aa817aa       49 minutes ago      Running             etcd                      2                   bd4e050f17afa       etcd-minikube
0c9e4e82cd0fe       2f6c962e7b831       24 hours ago        Exited              coredns                   1                   fc2ed70e57c0a       coredns-668d6bf9bc-84752
d69635b32bba4       2f50386e20bfd       24 hours ago        Exited              kube-proxy                1                   05719ce1bf76d       kube-proxy-gdwpw
923d2abb24abe       2b5bd0f16085a       24 hours ago        Exited              kube-apiserver            1                   8b5f077b681b2       kube-apiserver-minikube
1faf88b6d71be       7fc9d4aa817aa       24 hours ago        Exited              etcd                      1                   e0fe8c61e1b7f       etcd-minikube
b4e74bcc16550       c3ff26fb59f37       24 hours ago        Exited              kube-scheduler            1                   9e60f2fba9ec6       kube-scheduler-minikube
7bc2a4be8cfe6       a8d049396f6b8       24 hours ago        Exited              kube-controller-manager   1                   1737484d50874       kube-controller-manager-minikube


==> coredns [0c9e4e82cd0f] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.11.3
linux/arm64, go1.21.11, a6338e9
[INFO] 127.0.0.1:60291 - 65396 "HINFO IN 2221732685754828612.9105007551783719970. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.075239458s
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[641362776]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (25-Feb-2025 12:51:37.069) (total time: 10008ms):
Trace[641362776]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10007ms (12:51:47.077)
Trace[641362776]: [10.008149172s] [10.008149172s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[965182418]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (25-Feb-2025 12:51:37.069) (total time: 10008ms):
Trace[965182418]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10008ms (12:51:47.077)
Trace[965182418]: [10.008470713s] [10.008470713s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[673912284]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (25-Feb-2025 12:51:37.069) (total time: 10008ms):
Trace[673912284]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10008ms (12:51:47.078)
Trace[673912284]: [10.00866338s] [10.00866338s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.479286001s


==> coredns [c0b63bd99392] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.11.3
linux/arm64, go1.21.11, a6338e9
[INFO] 127.0.0.1:34780 - 5235 "HINFO IN 6449455512376569485.5026627505697372301. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.045564625s
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[844063033]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (26-Feb-2025 11:53:21.393) (total time: 10008ms):
Trace[844063033]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10008ms (11:53:31.401)
Trace[844063033]: [10.008639962s] [10.008639962s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[213456588]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (26-Feb-2025 11:53:21.393) (total time: 10009ms):
Trace[213456588]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10008ms (11:53:31.402)
Trace[213456588]: [10.009442004s] [10.009442004s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[337865353]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (26-Feb-2025 11:53:21.393) (total time: 10010ms):
Trace[337865353]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10010ms (11:53:31.403)
Trace[337865353]: [10.010341254s] [10.010341254s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=arm64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=arm64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_02_25T16_40_35_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 25 Feb 2025 12:40:32 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Wed, 26 Feb 2025 12:42:17 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 26 Feb 2025 12:38:29 +0000   Tue, 25 Feb 2025 12:40:31 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 26 Feb 2025 12:38:29 +0000   Tue, 25 Feb 2025 12:40:31 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 26 Feb 2025 12:38:29 +0000   Tue, 25 Feb 2025 12:40:31 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 26 Feb 2025 12:38:29 +0000   Tue, 25 Feb 2025 12:40:33 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                11
  ephemeral-storage:  1055761844Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             8024836Ki
  pods:               110
Allocatable:
  cpu:                11
  ephemeral-storage:  1055761844Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             8024836Ki
  pods:               110
System Info:
  Machine ID:                 d1748707be544d10a81d36fbff67a1e6
  System UUID:                d1748707be544d10a81d36fbff67a1e6
  Boot ID:                    1b9e8529-0589-438f-910e-ca9baf0bda47
  Kernel Version:             6.12.5-linuxkit
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               arm64
  Container Runtime Version:  docker://27.4.1
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (12 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     angular-app-66c6dfc877-p5jr9                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         48m
  default                     hello-minikube-ffcbb5874-x8lsk                0 (0%)        0 (0%)      0 (0%)           0 (0%)         23h
  default                     hello-minikube1-68d8f56889-wsf5f              0 (0%)        0 (0%)      0 (0%)           0 (0%)         23h
  kube-system                 coredns-668d6bf9bc-84752                      100m (0%)     0 (0%)      70Mi (0%)        170Mi (2%)     24h
  kube-system                 etcd-minikube                                 100m (0%)     0 (0%)      100Mi (1%)       0 (0%)         24h
  kube-system                 kube-apiserver-minikube                       250m (2%)     0 (0%)      0 (0%)           0 (0%)         24h
  kube-system                 kube-controller-manager-minikube              200m (1%)     0 (0%)      0 (0%)           0 (0%)         24h
  kube-system                 kube-proxy-gdwpw                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         24h
  kube-system                 kube-scheduler-minikube                       100m (0%)     0 (0%)      0 (0%)           0 (0%)         24h
  kube-system                 storage-provisioner                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         24h
  kubernetes-dashboard        dashboard-metrics-scraper-5d59dccf9b-mqv47    0 (0%)        0 (0%)      0 (0%)           0 (0%)         23h
  kubernetes-dashboard        kubernetes-dashboard-7779f9b69b-jw9nx         0 (0%)        0 (0%)      0 (0%)           0 (0%)         23h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (6%)   0 (0%)
  memory             170Mi (2%)  170Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
  hugepages-32Mi     0 (0%)      0 (0%)
  hugepages-64Ki     0 (0%)      0 (0%)
Events:
  Type     Reason                   Age                From             Message
  ----     ------                   ----               ----             -------
  Normal   Starting                 48m                kube-proxy       
  Normal   Starting                 49m                kubelet          Starting kubelet.
  Normal   NodeHasSufficientMemory  49m (x8 over 49m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    49m (x8 over 49m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     49m (x7 over 49m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  49m                kubelet          Updated Node Allocatable limit across pods
  Warning  Rebooted                 49m                kubelet          Node minikube has been rebooted, boot id: 1b9e8529-0589-438f-910e-ca9baf0bda47
  Normal   RegisteredNode           49m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Feb26 11:33] netlink: 'init': attribute type 4 has an invalid length.
[  +0.126105] fakeowner: loading out-of-tree module taints kernel.
[Feb26 11:53] psi: inconsistent task state! task=4739:containerd-shim cpu=5 psi_flags=14 clear=0 set=4


==> etcd [1faf88b6d71b] <==
{"level":"info","ts":"2025-02-25T19:26:26.438740Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":7370,"took":"1.906208ms","hash":1321087225,"current-db-size-bytes":3538944,"current-db-size":"3.5 MB","current-db-size-in-use-bytes":1740800,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-02-25T19:26:26.438784Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1321087225,"revision":7370,"compact-revision":7115}
{"level":"info","ts":"2025-02-25T19:40:50.172141Z","caller":"traceutil/trace.go:171","msg":"trace[1854846783] transaction","detail":"{read_only:false; response_revision:7716; number_of_response:1; }","duration":"111.196875ms","start":"2025-02-25T19:40:50.060932Z","end":"2025-02-25T19:40:50.172129Z","steps":["trace[1854846783] 'process raft request'  (duration: 111.050917ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-25T19:40:50.172224Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"110.390791ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-02-25T19:40:50.172239Z","caller":"traceutil/trace.go:171","msg":"trace[2047270382] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:7716; }","duration":"110.426333ms","start":"2025-02-25T19:40:50.061809Z","end":"2025-02-25T19:40:50.172235Z","steps":["trace[2047270382] 'agreement among raft nodes before linearized reading'  (duration: 110.374791ms)"],"step_count":1}
{"level":"info","ts":"2025-02-25T19:40:50.172114Z","caller":"traceutil/trace.go:171","msg":"trace[726993929] linearizableReadLoop","detail":"{readStateIndex:9517; appliedIndex:9516; }","duration":"110.277208ms","start":"2025-02-25T19:40:50.061810Z","end":"2025-02-25T19:40:50.172088Z","steps":["trace[726993929] 'read index received'  (duration: 110.104291ms)","trace[726993929] 'applied index is now lower than readState.Index'  (duration: 172.5¬µs)"],"step_count":2}
{"level":"info","ts":"2025-02-25T20:37:16.375823Z","caller":"traceutil/trace.go:171","msg":"trace[858099975] transaction","detail":"{read_only:false; response_revision:7731; number_of_response:1; }","duration":"106.146791ms","start":"2025-02-25T20:37:16.269657Z","end":"2025-02-25T20:37:16.375804Z","steps":["trace[858099975] 'process raft request'  (duration: 105.962875ms)"],"step_count":1}
{"level":"info","ts":"2025-02-25T20:37:16.375878Z","caller":"traceutil/trace.go:171","msg":"trace[1570921512] linearizableReadLoop","detail":"{readStateIndex:9536; appliedIndex:9536; }","duration":"105.816833ms","start":"2025-02-25T20:37:16.270052Z","end":"2025-02-25T20:37:16.375869Z","steps":["trace[1570921512] 'read index received'  (duration: 105.809041ms)","trace[1570921512] 'applied index is now lower than readState.Index'  (duration: 6.417¬µs)"],"step_count":2}
{"level":"warn","ts":"2025-02-25T20:37:16.375960Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"105.889417ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-02-25T20:37:16.375980Z","caller":"traceutil/trace.go:171","msg":"trace[963031991] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:7731; }","duration":"105.92575ms","start":"2025-02-25T20:37:16.270049Z","end":"2025-02-25T20:37:16.375975Z","steps":["trace[963031991] 'agreement among raft nodes before linearized reading'  (duration: 105.862541ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-25T20:37:16.376163Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"106.006083ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/ranges/servicenodeports\" limit:1 ","response":"range_response_count:1 size:446"}
{"level":"info","ts":"2025-02-25T20:37:16.376172Z","caller":"traceutil/trace.go:171","msg":"trace[2146652970] range","detail":"{range_begin:/registry/ranges/servicenodeports; range_end:; response_count:1; response_revision:7732; }","duration":"106.018708ms","start":"2025-02-25T20:37:16.270152Z","end":"2025-02-25T20:37:16.376170Z","steps":["trace[2146652970] 'agreement among raft nodes before linearized reading'  (duration: 105.953167ms)"],"step_count":1}
{"level":"info","ts":"2025-02-25T20:37:16.376182Z","caller":"traceutil/trace.go:171","msg":"trace[76337572] transaction","detail":"{read_only:false; response_revision:7732; number_of_response:1; }","duration":"106.421167ms","start":"2025-02-25T20:37:16.269756Z","end":"2025-02-25T20:37:16.376177Z","steps":["trace[76337572] 'process raft request'  (duration: 106.31775ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-25T20:37:16.376322Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"103.748875ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/kubernetes-dashboard/kubernetes-dashboard-7779f9b69b-jw9nx\" limit:1 ","response":"range_response_count:1 size:4551"}
{"level":"warn","ts":"2025-02-25T20:37:16.376359Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"106.165333ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/ranges/serviceips\" limit:1 ","response":"range_response_count:1 size:107835"}
{"level":"info","ts":"2025-02-25T20:37:16.376367Z","caller":"traceutil/trace.go:171","msg":"trace[2096403866] range","detail":"{range_begin:/registry/ranges/serviceips; range_end:; response_count:1; response_revision:7732; }","duration":"106.175417ms","start":"2025-02-25T20:37:16.270189Z","end":"2025-02-25T20:37:16.376364Z","steps":["trace[2096403866] 'agreement among raft nodes before linearized reading'  (duration: 106.119292ms)"],"step_count":1}
{"level":"info","ts":"2025-02-25T20:37:16.376330Z","caller":"traceutil/trace.go:171","msg":"trace[1547024573] range","detail":"{range_begin:/registry/pods/kubernetes-dashboard/kubernetes-dashboard-7779f9b69b-jw9nx; range_end:; response_count:1; response_revision:7732; }","duration":"103.75575ms","start":"2025-02-25T20:37:16.272570Z","end":"2025-02-25T20:37:16.376326Z","steps":["trace[1547024573] 'agreement among raft nodes before linearized reading'  (duration: 103.725208ms)"],"step_count":1}
{"level":"info","ts":"2025-02-25T21:43:57.036005Z","caller":"traceutil/trace.go:171","msg":"trace[1675306853] transaction","detail":"{read_only:false; response_revision:7768; number_of_response:1; }","duration":"117.277625ms","start":"2025-02-25T21:43:56.918713Z","end":"2025-02-25T21:43:57.035991Z","steps":["trace[1675306853] 'process raft request'  (duration: 117.181709ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-25T22:54:16.222759Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"111.30725ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-02-25T22:54:16.222842Z","caller":"traceutil/trace.go:171","msg":"trace[565860619] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:7814; }","duration":"111.402708ms","start":"2025-02-25T22:54:16.111425Z","end":"2025-02-25T22:54:16.222827Z","steps":["trace[565860619] 'range keys from in-memory index tree'  (duration: 111.249667ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-25T22:54:16.222759Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"113.992791ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-02-25T22:54:16.223007Z","caller":"traceutil/trace.go:171","msg":"trace[734290167] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:7814; }","duration":"114.2615ms","start":"2025-02-25T22:54:16.108739Z","end":"2025-02-25T22:54:16.223001Z","steps":["trace[734290167] 'range keys from in-memory index tree'  (duration: 113.978875ms)"],"step_count":1}
{"level":"info","ts":"2025-02-26T00:17:28.218976Z","caller":"traceutil/trace.go:171","msg":"trace[1225533860] transaction","detail":"{read_only:false; response_revision:7827; number_of_response:1; }","duration":"110.464875ms","start":"2025-02-26T00:17:28.108492Z","end":"2025-02-26T00:17:28.218957Z","steps":["trace[1225533860] 'process raft request'  (duration: 110.330375ms)"],"step_count":1}
{"level":"info","ts":"2025-02-26T01:18:09.371450Z","caller":"traceutil/trace.go:171","msg":"trace[73662992] transaction","detail":"{read_only:false; response_revision:7862; number_of_response:1; }","duration":"117.698ms","start":"2025-02-26T01:18:09.253722Z","end":"2025-02-26T01:18:09.371420Z","steps":["trace[73662992] 'process raft request'  (duration: 117.620834ms)"],"step_count":1}
{"level":"info","ts":"2025-02-26T01:18:20.004136Z","caller":"traceutil/trace.go:171","msg":"trace[2023209813] transaction","detail":"{read_only:false; response_revision:7871; number_of_response:1; }","duration":"116.587625ms","start":"2025-02-26T01:18:19.887534Z","end":"2025-02-26T01:18:20.004122Z","steps":["trace[2023209813] 'process raft request'  (duration: 116.350084ms)"],"step_count":1}
{"level":"info","ts":"2025-02-26T02:27:57.614869Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7626}
{"level":"info","ts":"2025-02-26T02:27:57.617538Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":7626,"took":"2.442041ms","hash":1099673162,"current-db-size-bytes":3538944,"current-db-size":"3.5 MB","current-db-size-in-use-bytes":1703936,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-02-26T02:27:57.617583Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1099673162,"revision":7626,"compact-revision":7370}
{"level":"info","ts":"2025-02-26T03:34:50.646252Z","caller":"traceutil/trace.go:171","msg":"trace[500024198] transaction","detail":"{read_only:false; response_revision:7911; number_of_response:1; }","duration":"102.535667ms","start":"2025-02-26T03:34:50.543702Z","end":"2025-02-26T03:34:50.646237Z","steps":["trace[500024198] 'process raft request'  (duration: 102.443959ms)"],"step_count":1}
{"level":"info","ts":"2025-02-26T03:34:56.905135Z","caller":"traceutil/trace.go:171","msg":"trace[483361286] linearizableReadLoop","detail":"{readStateIndex:9772; appliedIndex:9771; }","duration":"114.223917ms","start":"2025-02-26T03:34:56.790892Z","end":"2025-02-26T03:34:56.905116Z","steps":["trace[483361286] 'read index received'  (duration: 114.129792ms)","trace[483361286] 'applied index is now lower than readState.Index'  (duration: 93.708¬µs)"],"step_count":2}
{"level":"warn","ts":"2025-02-26T03:34:56.905232Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"114.320959ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-02-26T03:34:56.905248Z","caller":"traceutil/trace.go:171","msg":"trace[883231873] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:7918; }","duration":"114.353709ms","start":"2025-02-26T03:34:56.790889Z","end":"2025-02-26T03:34:56.905243Z","steps":["trace[883231873] 'agreement among raft nodes before linearized reading'  (duration: 114.298875ms)"],"step_count":1}
{"level":"info","ts":"2025-02-26T03:34:56.905468Z","caller":"traceutil/trace.go:171","msg":"trace[2008786575] transaction","detail":"{read_only:false; response_revision:7918; number_of_response:1; }","duration":"115.019625ms","start":"2025-02-26T03:34:56.790445Z","end":"2025-02-26T03:34:56.905465Z","steps":["trace[2008786575] 'process raft request'  (duration: 114.579333ms)"],"step_count":1}
{"level":"info","ts":"2025-02-26T06:14:08.258752Z","caller":"traceutil/trace.go:171","msg":"trace[1825460519] linearizableReadLoop","detail":"{readStateIndex:9891; appliedIndex:9890; }","duration":"114.97725ms","start":"2025-02-26T06:14:08.143757Z","end":"2025-02-26T06:14:08.258734Z","steps":["trace[1825460519] 'read index received'  (duration: 114.899666ms)","trace[1825460519] 'applied index is now lower than readState.Index'  (duration: 77.042¬µs)"],"step_count":2}
{"level":"warn","ts":"2025-02-26T06:14:08.258841Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"115.067375ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-02-26T06:14:08.258858Z","caller":"traceutil/trace.go:171","msg":"trace[1293206195] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:8012; }","duration":"115.097083ms","start":"2025-02-26T06:14:08.143755Z","end":"2025-02-26T06:14:08.258852Z","steps":["trace[1293206195] 'agreement among raft nodes before linearized reading'  (duration: 115.050458ms)"],"step_count":1}
{"level":"info","ts":"2025-02-26T06:32:32.483416Z","caller":"etcdserver/server.go:1473","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":10001,"local-member-snapshot-index":0,"local-member-snapshot-count":10000}
{"level":"info","ts":"2025-02-26T06:32:32.488888Z","caller":"etcdserver/server.go:2493","msg":"saved snapshot","snapshot-index":10001}
{"level":"info","ts":"2025-02-26T06:32:32.489030Z","caller":"etcdserver/server.go:2523","msg":"compacted Raft logs","compact-index":5001}
{"level":"info","ts":"2025-02-26T06:33:13.917787Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7881}
{"level":"info","ts":"2025-02-26T06:33:13.919878Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":7881,"took":"1.8795ms","hash":4218877211,"current-db-size-bytes":3538944,"current-db-size":"3.5 MB","current-db-size-in-use-bytes":1720320,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-02-26T06:33:13.919923Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":4218877211,"revision":7881,"compact-revision":7626}
{"level":"info","ts":"2025-02-26T06:38:13.909044Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8135}
{"level":"info","ts":"2025-02-26T06:38:13.914932Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":8135,"took":"5.629958ms","hash":589366402,"current-db-size-bytes":3538944,"current-db-size":"3.5 MB","current-db-size-in-use-bytes":1716224,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-02-26T06:38:13.915009Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":589366402,"revision":8135,"compact-revision":7881}
{"level":"info","ts":"2025-02-26T06:43:13.917063Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8391}
{"level":"info","ts":"2025-02-26T06:43:13.918904Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":8391,"took":"1.676792ms","hash":2408309801,"current-db-size-bytes":3538944,"current-db-size":"3.5 MB","current-db-size-in-use-bytes":1748992,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-02-26T06:43:13.918940Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2408309801,"revision":8391,"compact-revision":8135}
{"level":"info","ts":"2025-02-26T06:51:22.196295Z","caller":"traceutil/trace.go:171","msg":"trace[939273287] transaction","detail":"{read_only:false; response_revision:8686; number_of_response:1; }","duration":"108.356208ms","start":"2025-02-26T06:51:22.087930Z","end":"2025-02-26T06:51:22.196287Z","steps":["trace[939273287] 'process raft request'  (duration: 108.233333ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-26T06:51:22.196345Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"107.856583ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-02-26T06:51:22.196359Z","caller":"traceutil/trace.go:171","msg":"trace[626579982] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:8686; }","duration":"107.885583ms","start":"2025-02-26T06:51:22.088471Z","end":"2025-02-26T06:51:22.196356Z","steps":["trace[626579982] 'agreement among raft nodes before linearized reading'  (duration: 107.8455ms)"],"step_count":1}
{"level":"info","ts":"2025-02-26T06:51:22.196273Z","caller":"traceutil/trace.go:171","msg":"trace[2135666722] linearizableReadLoop","detail":"{readStateIndex:10742; appliedIndex:10741; }","duration":"107.782833ms","start":"2025-02-26T06:51:22.088472Z","end":"2025-02-26T06:51:22.196255Z","steps":["trace[2135666722] 'read index received'  (duration: 107.664291ms)","trace[2135666722] 'applied index is now lower than readState.Index'  (duration: 118.084¬µs)"],"step_count":2}
{"level":"info","ts":"2025-02-26T06:56:12.789600Z","caller":"traceutil/trace.go:171","msg":"trace[672335406] transaction","detail":"{read_only:false; response_revision:8695; number_of_response:1; }","duration":"101.910708ms","start":"2025-02-26T06:56:12.687671Z","end":"2025-02-26T06:56:12.789582Z","steps":["trace[672335406] 'process raft request'  (duration: 101.846208ms)"],"step_count":1}
{"level":"info","ts":"2025-02-26T06:56:12.789707Z","caller":"traceutil/trace.go:171","msg":"trace[1420939465] linearizableReadLoop","detail":"{readStateIndex:10753; appliedIndex:10752; }","duration":"104.9595ms","start":"2025-02-26T06:56:12.684738Z","end":"2025-02-26T06:56:12.789698Z","steps":["trace[1420939465] 'read index received'  (duration: 104.234ms)","trace[1420939465] 'applied index is now lower than readState.Index'  (duration: 724.834¬µs)"],"step_count":2}
{"level":"warn","ts":"2025-02-26T06:56:12.789823Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"105.066458ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 ","response":"range_response_count:1 size:1109"}
{"level":"info","ts":"2025-02-26T06:56:12.789841Z","caller":"traceutil/trace.go:171","msg":"trace[834408821] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:8695; }","duration":"105.102583ms","start":"2025-02-26T06:56:12.684732Z","end":"2025-02-26T06:56:12.789835Z","steps":["trace[834408821] 'agreement among raft nodes before linearized reading'  (duration: 105.007375ms)"],"step_count":1}
{"level":"info","ts":"2025-02-26T06:56:25.880939Z","caller":"traceutil/trace.go:171","msg":"trace[1592686794] linearizableReadLoop","detail":"{readStateIndex:10769; appliedIndex:10768; }","duration":"115.6195ms","start":"2025-02-26T06:56:25.765307Z","end":"2025-02-26T06:56:25.880926Z","steps":["trace[1592686794] 'read index received'  (duration: 115.507875ms)","trace[1592686794] 'applied index is now lower than readState.Index'  (duration: 111.5¬µs)"],"step_count":2}
{"level":"info","ts":"2025-02-26T06:56:25.881075Z","caller":"traceutil/trace.go:171","msg":"trace[258839767] transaction","detail":"{read_only:false; response_revision:8707; number_of_response:1; }","duration":"116.366792ms","start":"2025-02-26T06:56:25.764704Z","end":"2025-02-26T06:56:25.881070Z","steps":["trace[258839767] 'process raft request'  (duration: 116.147375ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-26T06:56:25.881189Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"115.864083ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-02-26T06:56:25.881202Z","caller":"traceutil/trace.go:171","msg":"trace[338787560] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:8707; }","duration":"115.893917ms","start":"2025-02-26T06:56:25.765306Z","end":"2025-02-26T06:56:25.881200Z","steps":["trace[338787560] 'agreement among raft nodes before linearized reading'  (duration: 115.853917ms)"],"step_count":1}


==> etcd [8581616c15f9] <==
{"level":"info","ts":"2025-02-26T11:53:16.300575Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-02-26T11:53:16.300591Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","recovered-remote-peer-id":"aec36adc501070cc","recovered-remote-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-02-26T11:53:16.300596Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2025-02-26T11:53:16.301151Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-02-26T11:53:16.301897Z","caller":"mvcc/kvstore.go:346","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":8391}
{"level":"info","ts":"2025-02-26T11:53:16.303589Z","caller":"mvcc/kvstore.go:423","msg":"kvstore restored","current-rev":8899}
{"level":"info","ts":"2025-02-26T11:53:16.304167Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-02-26T11:53:16.304815Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2025-02-26T11:53:16.305055Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-02-26T11:53:16.305074Z","caller":"etcdserver/server.go:864","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.16","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2025-02-26T11:53:16.305132Z","caller":"etcdserver/server.go:757","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2025-02-26T11:53:16.305358Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-02-26T11:53:16.306023Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-02-26T11:53:16.306054Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-02-26T11:53:16.306058Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-02-26T11:53:16.306110Z","caller":"embed/etcd.go:729","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-02-26T11:53:16.306230Z","caller":"embed/etcd.go:280","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-02-26T11:53:16.306242Z","caller":"embed/etcd.go:871","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-02-26T11:53:16.306444Z","caller":"embed/etcd.go:600","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-02-26T11:53:16.306453Z","caller":"embed/etcd.go:572","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-02-26T11:53:17.301020Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 3"}
{"level":"info","ts":"2025-02-26T11:53:17.301112Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 3"}
{"level":"info","ts":"2025-02-26T11:53:17.301185Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2025-02-26T11:53:17.301196Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 4"}
{"level":"info","ts":"2025-02-26T11:53:17.301208Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 4"}
{"level":"info","ts":"2025-02-26T11:53:17.301215Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 4"}
{"level":"info","ts":"2025-02-26T11:53:17.301222Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 4"}
{"level":"info","ts":"2025-02-26T11:53:17.302714Z","caller":"etcdserver/server.go:2140","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-02-26T11:53:17.302865Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-02-26T11:53:17.303023Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-02-26T11:53:17.303156Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-02-26T11:53:17.303178Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-02-26T11:53:17.303848Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-02-26T11:53:17.303857Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-02-26T11:53:17.304297Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-02-26T11:53:17.304383Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-02-26T12:03:17.326633Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":9555}
{"level":"info","ts":"2025-02-26T12:03:17.340324Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":9555,"took":"13.357166ms","hash":2919789374,"current-db-size-bytes":3825664,"current-db-size":"3.8 MB","current-db-size-in-use-bytes":2142208,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2025-02-26T12:03:17.340386Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2919789374,"revision":9555,"compact-revision":8391}
{"level":"info","ts":"2025-02-26T12:08:17.337156Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":9817}
{"level":"info","ts":"2025-02-26T12:08:17.340663Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":9817,"took":"3.049291ms","hash":2130565143,"current-db-size-bytes":3825664,"current-db-size":"3.8 MB","current-db-size-in-use-bytes":1867776,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-02-26T12:08:17.340740Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2130565143,"revision":9817,"compact-revision":9555}
{"level":"info","ts":"2025-02-26T12:13:17.346752Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":10080}
{"level":"info","ts":"2025-02-26T12:13:17.349970Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":10080,"took":"2.898583ms","hash":122805755,"current-db-size-bytes":3825664,"current-db-size":"3.8 MB","current-db-size-in-use-bytes":1839104,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-02-26T12:13:17.350020Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":122805755,"revision":10080,"compact-revision":9817}
{"level":"info","ts":"2025-02-26T12:18:17.356042Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":10339}
{"level":"info","ts":"2025-02-26T12:18:17.359195Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":10339,"took":"2.823083ms","hash":3746982077,"current-db-size-bytes":3825664,"current-db-size":"3.8 MB","current-db-size-in-use-bytes":1843200,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-02-26T12:18:17.359270Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3746982077,"revision":10339,"compact-revision":10080}
{"level":"info","ts":"2025-02-26T12:23:17.362527Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":10598}
{"level":"info","ts":"2025-02-26T12:23:17.364956Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":10598,"took":"2.118834ms","hash":1979616250,"current-db-size-bytes":3825664,"current-db-size":"3.8 MB","current-db-size-in-use-bytes":1859584,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-02-26T12:23:17.365044Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1979616250,"revision":10598,"compact-revision":10339}
{"level":"info","ts":"2025-02-26T12:28:17.371691Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":10859}
{"level":"info","ts":"2025-02-26T12:28:17.374442Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":10859,"took":"2.414667ms","hash":3834989994,"current-db-size-bytes":3825664,"current-db-size":"3.8 MB","current-db-size-in-use-bytes":1847296,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-02-26T12:28:17.374506Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3834989994,"revision":10859,"compact-revision":10598}
{"level":"info","ts":"2025-02-26T12:33:17.380489Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11118}
{"level":"info","ts":"2025-02-26T12:33:17.382907Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":11118,"took":"2.292625ms","hash":1868492921,"current-db-size-bytes":3825664,"current-db-size":"3.8 MB","current-db-size-in-use-bytes":1863680,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-02-26T12:33:17.382962Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1868492921,"revision":11118,"compact-revision":10859}
{"level":"info","ts":"2025-02-26T12:38:17.387584Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11377}
{"level":"info","ts":"2025-02-26T12:38:17.391125Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":11377,"took":"3.1945ms","hash":2770285456,"current-db-size-bytes":3825664,"current-db-size":"3.8 MB","current-db-size-in-use-bytes":1867776,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-02-26T12:38:17.391202Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2770285456,"revision":11377,"compact-revision":11118}


==> kernel <==
 12:42:21 up  1:08,  0 users,  load average: 2.50, 2.65, 2.59
Linux minikube 6.12.5-linuxkit #1 SMP Tue Jan 21 10:23:32 UTC 2025 aarch64 aarch64 aarch64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [73b77df22734] <==
I0226 11:53:17.711243       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0226 11:53:17.711256       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0226 11:53:17.711249       1 controller.go:119] Starting legacy_token_tracking_controller
I0226 11:53:17.711265       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0226 11:53:17.711995       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0226 11:53:17.712009       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0226 11:53:17.712010       1 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0226 11:53:17.712498       1 local_available_controller.go:156] Starting LocalAvailability controller
I0226 11:53:17.712509       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0226 11:53:17.712525       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0226 11:53:17.712553       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0226 11:53:17.712559       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0226 11:53:17.716360       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0226 11:53:17.712003       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0226 11:53:17.716461       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I0226 11:53:17.716473       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0226 11:53:17.716475       1 aggregator.go:169] waiting for initial CRD sync...
I0226 11:53:17.716482       1 controller.go:78] Starting OpenAPI AggregationController
I0226 11:53:17.716828       1 controller.go:142] Starting OpenAPI controller
I0226 11:53:17.716915       1 controller.go:90] Starting OpenAPI V3 controller
I0226 11:53:17.716948       1 naming_controller.go:294] Starting NamingConditionController
I0226 11:53:17.716999       1 cluster_authentication_trust_controller.go:462] Starting cluster_authentication_trust_controller controller
I0226 11:53:17.717004       1 establishing_controller.go:81] Starting EstablishingController
I0226 11:53:17.717027       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0226 11:53:17.717036       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0226 11:53:17.717057       1 crd_finalizer.go:269] Starting CRDFinalizer
I0226 11:53:17.711999       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0226 11:53:17.717003       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0226 11:53:17.717007       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0226 11:53:17.717246       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0226 11:53:17.717260       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0226 11:53:17.717288       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0226 11:53:17.717352       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0226 11:53:17.730183       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0226 11:53:17.730199       1 policy_source.go:240] refreshing policies
I0226 11:53:17.811725       1 shared_informer.go:320] Caches are synced for configmaps
I0226 11:53:17.811739       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0226 11:53:17.812546       1 cache.go:39] Caches are synced for LocalAvailability controller
I0226 11:53:17.812597       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0226 11:53:17.814413       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0226 11:53:17.817929       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0226 11:53:17.817939       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0226 11:53:17.817990       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0226 11:53:17.818055       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0226 11:53:17.818080       1 aggregator.go:171] initial CRD sync complete...
I0226 11:53:17.818086       1 autoregister_controller.go:144] Starting autoregister controller
I0226 11:53:17.818099       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0226 11:53:17.818104       1 cache.go:39] Caches are synced for autoregister controller
E0226 11:53:17.821141       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0226 11:53:17.826165       1 shared_informer.go:320] Caches are synced for node_authorizer
I0226 11:53:17.829144       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
E0226 11:53:17.830658       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: 06697ace-3ec0-4e35-b7c3-f6597864130a, UID in object meta: "
I0226 11:53:18.721086       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0226 11:53:18.874406       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0226 11:53:20.560436       1 controller.go:615] quota admission added evaluator for: endpoints
I0226 11:53:20.560870       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0226 11:53:20.567766       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0226 11:53:20.581308       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0226 11:53:20.612704       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0226 11:53:41.393152       1 alloc.go:330] "allocated clusterIPs" service="default/angular-app-service" clusterIPs={"IPv4":"10.98.251.216"}


==> kube-apiserver [923d2abb24ab] <==
I0225 12:51:35.916022       1 controller.go:78] Starting OpenAPI AggregationController
I0225 12:51:35.916034       1 controller.go:142] Starting OpenAPI controller
I0225 12:51:35.916044       1 controller.go:90] Starting OpenAPI V3 controller
I0225 12:51:35.916071       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0225 12:51:35.931953       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0225 12:51:35.932056       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0225 12:51:35.932059       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0225 12:51:35.932166       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0225 12:51:35.932230       1 cluster_authentication_trust_controller.go:462] Starting cluster_authentication_trust_controller controller
I0225 12:51:35.932237       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0225 12:51:35.932244       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0225 12:51:36.061880       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0225 12:51:36.061925       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0225 12:51:36.061934       1 policy_source.go:240] refreshing policies
I0225 12:51:36.061975       1 aggregator.go:171] initial CRD sync complete...
I0225 12:51:36.061982       1 autoregister_controller.go:144] Starting autoregister controller
I0225 12:51:36.061990       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0225 12:51:36.061994       1 cache.go:39] Caches are synced for autoregister controller
I0225 12:51:36.062099       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0225 12:51:36.062133       1 shared_informer.go:320] Caches are synced for configmaps
I0225 12:51:36.062184       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0225 12:51:36.062335       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0225 12:51:36.062428       1 cache.go:39] Caches are synced for LocalAvailability controller
I0225 12:51:36.062836       1 shared_informer.go:320] Caches are synced for node_authorizer
I0225 12:51:36.062855       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0225 12:51:36.062859       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0225 12:51:36.066154       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
E0225 12:51:36.067699       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0225 12:51:36.075939       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0225 12:51:36.256160       1 controller.go:615] quota admission added evaluator for: endpoints
I0225 12:51:36.688817       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0225 12:51:36.919208       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0225 12:51:39.277600       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0225 12:51:39.529040       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0225 12:51:39.633050       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0225 12:51:41.687019       1 alloc.go:330] "allocated clusterIPs" service="default/hello-minikube1" clusterIPs={"IPv4":"10.109.38.4"}
I0225 12:53:17.965362       1 controller.go:615] quota admission added evaluator for: namespaces
I0225 12:53:17.980250       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0225 12:53:17.983397       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0225 12:53:18.003307       1 alloc.go:330] "allocated clusterIPs" service="kubernetes-dashboard/kubernetes-dashboard" clusterIPs={"IPv4":"10.106.191.208"}
I0225 12:53:18.010883       1 alloc.go:330] "allocated clusterIPs" service="kubernetes-dashboard/dashboard-metrics-scraper" clusterIPs={"IPv4":"10.101.69.3"}
I0225 12:54:48.036186       1 alloc.go:330] "allocated clusterIPs" service="default/hello-minikube" clusterIPs={"IPv4":"10.106.88.111"}
E0225 15:30:19.660627       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0225 15:30:19.661084       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0225 18:08:18.330822       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0225 18:08:18.330822       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0225 19:20:42.765416       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0225 19:20:42.765416       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0225 20:37:20.694667       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0225 20:37:20.694980       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0225 22:49:48.041007       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0225 22:49:48.041339       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0226 00:17:37.539341       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0226 00:17:37.539880       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0226 02:28:03.007851       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0226 02:28:03.007851       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0226 05:21:02.584193       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0226 05:21:02.584221       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0226 07:06:29.265577       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0226 07:06:29.265792       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"


==> kube-controller-manager [7bc2a4be8cfe] <==
I0225 19:14:20.496371       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube-ffcbb5874" duration="101.917¬µs"
I0225 19:20:42.766101       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E0225 19:20:42.766276       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I0225 19:25:33.540098       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0225 19:25:36.147869       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube1-68d8f56889" duration="139.75¬µs"
I0225 19:25:47.139754       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube1-68d8f56889" duration="68.625¬µs"
I0225 19:26:46.147517       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="96.917¬µs"
I0225 19:26:52.148953       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="114.333¬µs"
I0225 19:26:59.142900       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="143.209¬µs"
I0225 19:27:07.144615       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="94.625¬µs"
E0225 20:37:20.695319       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I0225 20:37:20.695384       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
I0225 20:37:38.082905       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube-ffcbb5874" duration="100.959¬µs"
I0225 21:43:48.298775       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube-ffcbb5874" duration="144.625¬µs"
I0225 22:49:48.041632       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E0225 22:49:48.041730       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
E0226 00:17:37.539860       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I0226 00:17:37.540275       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
I0226 00:17:45.659844       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 00:17:49.875142       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube1-68d8f56889" duration="111.25¬µs"
I0226 01:18:01.643235       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube1-68d8f56889" duration="164.417¬µs"
I0226 02:28:03.008485       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E0226 02:28:03.008566       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I0226 03:34:50.648209       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="56.333¬µs"
I0226 03:34:54.434422       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="190¬µs"
I0226 03:35:05.427298       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="41.417¬µs"
I0226 03:35:09.437195       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="89.875¬µs"
I0226 05:21:02.584616       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E0226 05:21:02.584665       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I0226 06:14:05.651548       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube-ffcbb5874" duration="125.041¬µs"
I0226 06:14:18.652104       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube-ffcbb5874" duration="109.333¬µs"
I0226 06:32:34.148493       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 06:32:51.633280       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube1-68d8f56889" duration="81.917¬µs"
I0226 06:33:03.638106       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube1-68d8f56889" duration="65.208¬µs"
I0226 06:34:05.597688       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="208.083¬µs"
I0226 06:34:05.605805       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="77.875¬µs"
I0226 06:34:17.595799       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="68.625¬µs"
I0226 06:34:19.595565       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="122.083¬µs"
I0226 06:35:46.596219       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube-ffcbb5874" duration="120.167¬µs"
I0226 06:35:59.594817       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube-ffcbb5874" duration="132.959¬µs"
I0226 06:37:40.153433       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 06:37:57.602551       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube1-68d8f56889" duration="96.917¬µs"
I0226 06:38:10.595213       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube1-68d8f56889" duration="68.959¬µs"
I0226 06:39:13.602853       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="64.458¬µs"
I0226 06:39:16.598406       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="87.209¬µs"
I0226 06:39:25.600992       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="77.125¬µs"
I0226 06:39:27.599165       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="90.916¬µs"
I0226 06:40:57.592105       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube-ffcbb5874" duration="118.333¬µs"
I0226 06:41:08.598779       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube-ffcbb5874" duration="168.583¬µs"
I0226 06:42:45.962913       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 06:43:13.598168       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube1-68d8f56889" duration="103.625¬µs"
I0226 06:43:25.602924       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube1-68d8f56889" duration="128.875¬µs"
I0226 06:56:17.635844       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="133.625¬µs"
I0226 06:56:21.637343       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="67.5¬µs"
I0226 06:56:30.639473       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="85¬µs"
I0226 06:56:33.638713       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="150.292¬µs"
E0226 07:06:29.266248       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I0226 07:06:29.266300       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
I0226 07:15:19.739915       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube-ffcbb5874" duration="143.042¬µs"
I0226 07:15:30.743030       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube-ffcbb5874" duration="108.208¬µs"


==> kube-controller-manager [e51553cf8a62] <==
I0226 12:14:46.566912       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube1-68d8f56889" duration="48.792¬µs"
I0226 12:14:48.563820       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-app-66c6dfc877" duration="86.208¬µs"
I0226 12:14:54.563385       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="121.083¬µs"
I0226 12:14:59.574999       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-app-66c6dfc877" duration="51.375¬µs"
I0226 12:15:08.568112       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="87.417¬µs"
I0226 12:18:05.701482       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 12:19:37.570208       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube1-68d8f56889" duration="106.916¬µs"
I0226 12:19:38.569173       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="158.542¬µs"
I0226 12:19:39.567842       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube-ffcbb5874" duration="86.667¬µs"
I0226 12:19:48.564758       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube1-68d8f56889" duration="187.708¬µs"
I0226 12:19:49.568262       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="91.5¬µs"
I0226 12:19:50.562853       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube-ffcbb5874" duration="46.209¬µs"
I0226 12:19:55.564459       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-app-66c6dfc877" duration="82.709¬µs"
I0226 12:19:59.569005       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="95.542¬µs"
I0226 12:20:07.577811       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-app-66c6dfc877" duration="74.75¬µs"
I0226 12:20:10.560121       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="76.042¬µs"
I0226 12:23:11.231558       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 12:24:34.568677       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="94.541¬µs"
I0226 12:24:37.572131       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube-ffcbb5874" duration="62¬µs"
I0226 12:24:47.570646       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="204.542¬µs"
I0226 12:24:52.571042       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube-ffcbb5874" duration="48.25¬µs"
I0226 12:24:53.568863       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube1-68d8f56889" duration="54.292¬µs"
I0226 12:25:02.566277       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="96.334¬µs"
I0226 12:25:04.571992       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-app-66c6dfc877" duration="82.958¬µs"
I0226 12:25:07.566873       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube1-68d8f56889" duration="43.75¬µs"
I0226 12:25:15.566488       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-app-66c6dfc877" duration="91.167¬µs"
I0226 12:25:17.571734       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="92.959¬µs"
I0226 12:28:16.406672       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 12:29:44.569364       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube-ffcbb5874" duration="95.792¬µs"
I0226 12:29:46.570173       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="108.792¬µs"
I0226 12:29:57.580482       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube-ffcbb5874" duration="73.041¬µs"
I0226 12:29:57.589896       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="197.375¬µs"
I0226 12:30:07.573602       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube1-68d8f56889" duration="49.5¬µs"
I0226 12:30:08.572527       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="95.792¬µs"
I0226 12:30:13.568320       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-app-66c6dfc877" duration="76.792¬µs"
I0226 12:30:19.569712       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube1-68d8f56889" duration="113.167¬µs"
I0226 12:30:23.581981       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="112.75¬µs"
I0226 12:30:24.572645       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-app-66c6dfc877" duration="50.625¬µs"
I0226 12:33:23.092258       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 12:34:53.573606       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube-ffcbb5874" duration="150.541¬µs"
I0226 12:34:55.574924       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="178.25¬µs"
I0226 12:35:08.574159       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube-ffcbb5874" duration="148.042¬µs"
I0226 12:35:10.568203       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="68.375¬µs"
I0226 12:35:13.574414       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube1-68d8f56889" duration="70.917¬µs"
I0226 12:35:15.573801       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="105.5¬µs"
I0226 12:35:22.574523       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-app-66c6dfc877" duration="52.584¬µs"
I0226 12:35:26.573505       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube1-68d8f56889" duration="69.792¬µs"
I0226 12:35:27.582394       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="114.959¬µs"
I0226 12:35:33.576991       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-app-66c6dfc877" duration="95.5¬µs"
I0226 12:38:29.230464       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0226 12:39:58.576944       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="94.917¬µs"
I0226 12:40:02.574745       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube-ffcbb5874" duration="34.041¬µs"
I0226 12:40:10.577658       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="60.959¬µs"
I0226 12:40:16.577893       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube-ffcbb5874" duration="135.375¬µs"
I0226 12:40:20.576868       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="125.834¬µs"
I0226 12:40:22.574752       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube1-68d8f56889" duration="105.125¬µs"
I0226 12:40:28.576297       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-app-66c6dfc877" duration="182.958¬µs"
I0226 12:40:32.585463       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="142.292¬µs"
I0226 12:40:36.574456       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube1-68d8f56889" duration="88.25¬µs"
I0226 12:40:43.574544       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/angular-app-66c6dfc877" duration="53.334¬µs"


==> kube-proxy [d69635b32bba] <==
I0225 12:51:37.079866       1 server_linux.go:66] "Using iptables proxy"
I0225 12:51:37.171177       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0225 12:51:37.171208       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0225 12:51:37.177700       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0225 12:51:37.177733       1 server_linux.go:170] "Using iptables Proxier"
I0225 12:51:37.178419       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0225 12:51:37.178545       1 server.go:497] "Version info" version="v1.32.0"
I0225 12:51:37.178555       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0225 12:51:37.179072       1 config.go:105] "Starting endpoint slice config controller"
I0225 12:51:37.179099       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0225 12:51:37.179128       1 config.go:199] "Starting service config controller"
I0225 12:51:37.179161       1 shared_informer.go:313] Waiting for caches to sync for service config
I0225 12:51:37.179173       1 config.go:329] "Starting node config controller"
I0225 12:51:37.179179       1 shared_informer.go:313] Waiting for caches to sync for node config
I0225 12:51:37.279192       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0225 12:51:37.279198       1 shared_informer.go:320] Caches are synced for service config
I0225 12:51:37.279760       1 shared_informer.go:320] Caches are synced for node config


==> kube-proxy [df5562402230] <==
I0226 11:53:21.341963       1 server_linux.go:66] "Using iptables proxy"
I0226 11:53:21.474832       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0226 11:53:21.474870       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0226 11:53:21.484095       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0226 11:53:21.484118       1 server_linux.go:170] "Using iptables Proxier"
I0226 11:53:21.484938       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0226 11:53:21.485698       1 server.go:497] "Version info" version="v1.32.0"
I0226 11:53:21.485710       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0226 11:53:21.487819       1 config.go:329] "Starting node config controller"
I0226 11:53:21.487823       1 config.go:105] "Starting endpoint slice config controller"
I0226 11:53:21.487956       1 config.go:199] "Starting service config controller"
I0226 11:53:21.488325       1 shared_informer.go:313] Waiting for caches to sync for node config
I0226 11:53:21.488324       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0226 11:53:21.488332       1 shared_informer.go:313] Waiting for caches to sync for service config
I0226 11:53:21.589971       1 shared_informer.go:320] Caches are synced for node config
I0226 11:53:21.591191       1 shared_informer.go:320] Caches are synced for service config
I0226 11:53:21.591206       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-scheduler [3e99af83255e] <==
I0226 11:53:16.792830       1 serving.go:386] Generated self-signed cert in-memory
W0226 11:53:16.868185       1 authentication.go:397] Error looking up in-cluster authentication configuration: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps/extension-apiserver-authentication": dial tcp 192.168.49.2:8443: connect: connection refused
W0226 11:53:16.868199       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0226 11:53:16.868202       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0226 11:53:16.874538       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0226 11:53:16.874551       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0226 11:53:16.876339       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0226 11:53:16.876393       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0226 11:53:16.876732       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
W0226 11:53:16.877175       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0226 11:53:16.877207       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W0226 11:53:16.877207       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0226 11:53:16.877224       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W0226 11:53:16.877254       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/volumeattachments?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0226 11:53:16.877265       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/volumeattachments?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
I0226 11:53:16.877292       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
W0226 11:53:16.877330       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0226 11:53:16.877342       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W0226 11:53:16.877628       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0226 11:53:16.877650       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W0226 11:53:16.877718       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0226 11:53:16.877721       1 reflector.go:569] runtime/asm_arm64.s:1223: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0226 11:53:16.877730       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get \"https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
E0226 11:53:16.877736       1 reflector.go:166] "Unhandled Error" err="runtime/asm_arm64.s:1223: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W0226 11:53:16.877742       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0226 11:53:16.877753       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get \"https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W0226 11:53:16.877774       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0226 11:53:16.877793       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get \"https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W0226 11:53:16.877820       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0226 11:53:16.878200       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0226 11:53:16.878230       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: Get \"https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W0226 11:53:16.878232       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0226 11:53:16.878245       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get \"https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
E0226 11:53:16.878277       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get \"https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W0226 11:53:16.878337       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0226 11:53:16.878354       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get \"https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W0226 11:53:16.878361       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0226 11:53:16.878373       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get \"https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W0226 11:53:16.878373       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0226 11:53:16.878387       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get \"https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W0226 11:53:16.878952       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0226 11:53:16.878988       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get \"https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
I0226 11:53:18.377174       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [b4e74bcc1655] <==
I0225 12:51:34.308404       1 serving.go:386] Generated self-signed cert in-memory
W0225 12:51:35.962958       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0225 12:51:35.962978       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0225 12:51:35.962983       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0225 12:51:35.962987       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0225 12:51:35.983839       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0225 12:51:35.983858       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0225 12:51:35.985248       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0225 12:51:35.985353       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0225 12:51:35.985366       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0225 12:51:35.985378       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0225 12:51:36.085862       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Feb 26 12:40:08 minikube kubelet[1524]: E0226 12:40:08.056696    1524 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" image="docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c"
Feb 26 12:40:08 minikube kubelet[1524]: E0226 12:40:08.056893    1524 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:dashboard-metrics-scraper,Image:docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:tmp-volume,ReadOnly:false,MountPath:/tmp,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-9z5xm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/,Port:{0 8000 },Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:30,TimeoutSeconds:30,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:*1001,RunAsNonRoot:nil,ReadOnlyRootFilesystem:*true,AllowPrivilegeEscalation:*false,RunAsGroup:*2001,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod dashboard-metrics-scraper-5d59dccf9b-mqv47_kubernetes-dashboard(067ad3f1-8946-4766-93a4-549d35de7555): ErrImagePull: Error response from daemon: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" logger="UnhandledError"
Feb 26 12:40:08 minikube kubelet[1524]: E0226 12:40:08.058117    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"dashboard-metrics-scraper\" with ErrImagePull: \"Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b-mqv47" podUID="067ad3f1-8946-4766-93a4-549d35de7555"
Feb 26 12:40:10 minikube kubelet[1524]: E0226 12:40:10.052037    1524 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" image="kicbase/echo-server:1.0"
Feb 26 12:40:10 minikube kubelet[1524]: E0226 12:40:10.052142    1524 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" image="kicbase/echo-server:1.0"
Feb 26 12:40:10 minikube kubelet[1524]: E0226 12:40:10.052374    1524 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:echo-server,Image:kicbase/echo-server:1.0,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6nb2q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-minikube1-68d8f56889-wsf5f_default(5dfaae61-dd8b-4785-8419-0d0d03ed9b0f): ErrImagePull: Error response from daemon: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" logger="UnhandledError"
Feb 26 12:40:10 minikube kubelet[1524]: E0226 12:40:10.055558    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"echo-server\" with ErrImagePull: \"Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="default/hello-minikube1-68d8f56889-wsf5f" podUID="5dfaae61-dd8b-4785-8419-0d0d03ed9b0f"
Feb 26 12:40:10 minikube kubelet[1524]: E0226 12:40:10.564303    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b-jw9nx" podUID="3dc2447b-9a6f-4705-8a16-5a64aec02a1c"
Feb 26 12:40:16 minikube kubelet[1524]: E0226 12:40:16.564884    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"echo-server\" with ImagePullBackOff: \"Back-off pulling image \\\"kicbase/echo-server:1.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="default/hello-minikube-ffcbb5874-x8lsk" podUID="c71de88b-066d-4e3c-9760-af200fdf204d"
Feb 26 12:40:17 minikube kubelet[1524]: E0226 12:40:17.048881    1524 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" image="angular-app:latest"
Feb 26 12:40:17 minikube kubelet[1524]: E0226 12:40:17.048944    1524 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" image="angular-app:latest"
Feb 26 12:40:17 minikube kubelet[1524]: E0226 12:40:17.049056    1524 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:angular-app,Image:angular-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hqlg7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod angular-app-66c6dfc877-p5jr9_default(685cb028-0c8d-43b9-a9e1-2077fa75a3f8): ErrImagePull: Error response from daemon: Get \"https://registry-1.docker.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" logger="UnhandledError"
Feb 26 12:40:17 minikube kubelet[1524]: E0226 12:40:17.051103    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"angular-app\" with ErrImagePull: \"Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="default/angular-app-66c6dfc877-p5jr9" podUID="685cb028-0c8d-43b9-a9e1-2077fa75a3f8"
Feb 26 12:40:20 minikube kubelet[1524]: E0226 12:40:20.564037    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"dashboard-metrics-scraper\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b-mqv47" podUID="067ad3f1-8946-4766-93a4-549d35de7555"
Feb 26 12:40:22 minikube kubelet[1524]: E0226 12:40:22.565860    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"echo-server\" with ImagePullBackOff: \"Back-off pulling image \\\"kicbase/echo-server:1.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="default/hello-minikube1-68d8f56889-wsf5f" podUID="5dfaae61-dd8b-4785-8419-0d0d03ed9b0f"
Feb 26 12:40:24 minikube kubelet[1524]: E0226 12:40:24.561529    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b-jw9nx" podUID="3dc2447b-9a6f-4705-8a16-5a64aec02a1c"
Feb 26 12:40:28 minikube kubelet[1524]: E0226 12:40:28.565538    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"echo-server\" with ImagePullBackOff: \"Back-off pulling image \\\"kicbase/echo-server:1.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="default/hello-minikube-ffcbb5874-x8lsk" podUID="c71de88b-066d-4e3c-9760-af200fdf204d"
Feb 26 12:40:28 minikube kubelet[1524]: E0226 12:40:28.565611    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"angular-app\" with ImagePullBackOff: \"Back-off pulling image \\\"angular-app:latest\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="default/angular-app-66c6dfc877-p5jr9" podUID="685cb028-0c8d-43b9-a9e1-2077fa75a3f8"
Feb 26 12:40:32 minikube kubelet[1524]: E0226 12:40:32.564810    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"dashboard-metrics-scraper\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b-mqv47" podUID="067ad3f1-8946-4766-93a4-549d35de7555"
Feb 26 12:40:35 minikube kubelet[1524]: E0226 12:40:35.563428    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b-jw9nx" podUID="3dc2447b-9a6f-4705-8a16-5a64aec02a1c"
Feb 26 12:40:36 minikube kubelet[1524]: E0226 12:40:36.564706    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"echo-server\" with ImagePullBackOff: \"Back-off pulling image \\\"kicbase/echo-server:1.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="default/hello-minikube1-68d8f56889-wsf5f" podUID="5dfaae61-dd8b-4785-8419-0d0d03ed9b0f"
Feb 26 12:40:40 minikube kubelet[1524]: E0226 12:40:40.562414    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"echo-server\" with ImagePullBackOff: \"Back-off pulling image \\\"kicbase/echo-server:1.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="default/hello-minikube-ffcbb5874-x8lsk" podUID="c71de88b-066d-4e3c-9760-af200fdf204d"
Feb 26 12:40:43 minikube kubelet[1524]: E0226 12:40:43.563881    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"angular-app\" with ImagePullBackOff: \"Back-off pulling image \\\"angular-app:latest\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="default/angular-app-66c6dfc877-p5jr9" podUID="685cb028-0c8d-43b9-a9e1-2077fa75a3f8"
Feb 26 12:40:43 minikube kubelet[1524]: E0226 12:40:43.564027    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"dashboard-metrics-scraper\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b-mqv47" podUID="067ad3f1-8946-4766-93a4-549d35de7555"
Feb 26 12:40:48 minikube kubelet[1524]: E0226 12:40:48.563514    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b-jw9nx" podUID="3dc2447b-9a6f-4705-8a16-5a64aec02a1c"
Feb 26 12:40:51 minikube kubelet[1524]: E0226 12:40:51.565208    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"echo-server\" with ImagePullBackOff: \"Back-off pulling image \\\"kicbase/echo-server:1.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="default/hello-minikube1-68d8f56889-wsf5f" podUID="5dfaae61-dd8b-4785-8419-0d0d03ed9b0f"
Feb 26 12:40:54 minikube kubelet[1524]: E0226 12:40:54.564222    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"echo-server\" with ImagePullBackOff: \"Back-off pulling image \\\"kicbase/echo-server:1.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="default/hello-minikube-ffcbb5874-x8lsk" podUID="c71de88b-066d-4e3c-9760-af200fdf204d"
Feb 26 12:40:55 minikube kubelet[1524]: E0226 12:40:55.566342    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"dashboard-metrics-scraper\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b-mqv47" podUID="067ad3f1-8946-4766-93a4-549d35de7555"
Feb 26 12:40:58 minikube kubelet[1524]: E0226 12:40:58.565549    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"angular-app\" with ImagePullBackOff: \"Back-off pulling image \\\"angular-app:latest\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="default/angular-app-66c6dfc877-p5jr9" podUID="685cb028-0c8d-43b9-a9e1-2077fa75a3f8"
Feb 26 12:41:01 minikube kubelet[1524]: E0226 12:41:01.563400    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b-jw9nx" podUID="3dc2447b-9a6f-4705-8a16-5a64aec02a1c"
Feb 26 12:41:05 minikube kubelet[1524]: E0226 12:41:05.575794    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"echo-server\" with ImagePullBackOff: \"Back-off pulling image \\\"kicbase/echo-server:1.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="default/hello-minikube1-68d8f56889-wsf5f" podUID="5dfaae61-dd8b-4785-8419-0d0d03ed9b0f"
Feb 26 12:41:08 minikube kubelet[1524]: E0226 12:41:08.565908    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"echo-server\" with ImagePullBackOff: \"Back-off pulling image \\\"kicbase/echo-server:1.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="default/hello-minikube-ffcbb5874-x8lsk" podUID="c71de88b-066d-4e3c-9760-af200fdf204d"
Feb 26 12:41:09 minikube kubelet[1524]: E0226 12:41:09.565629    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"dashboard-metrics-scraper\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b-mqv47" podUID="067ad3f1-8946-4766-93a4-549d35de7555"
Feb 26 12:41:13 minikube kubelet[1524]: E0226 12:41:13.566136    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"angular-app\" with ImagePullBackOff: \"Back-off pulling image \\\"angular-app:latest\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="default/angular-app-66c6dfc877-p5jr9" podUID="685cb028-0c8d-43b9-a9e1-2077fa75a3f8"
Feb 26 12:41:14 minikube kubelet[1524]: E0226 12:41:14.564749    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b-jw9nx" podUID="3dc2447b-9a6f-4705-8a16-5a64aec02a1c"
Feb 26 12:41:19 minikube kubelet[1524]: E0226 12:41:19.563086    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"echo-server\" with ImagePullBackOff: \"Back-off pulling image \\\"kicbase/echo-server:1.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="default/hello-minikube-ffcbb5874-x8lsk" podUID="c71de88b-066d-4e3c-9760-af200fdf204d"
Feb 26 12:41:20 minikube kubelet[1524]: E0226 12:41:20.565951    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"echo-server\" with ImagePullBackOff: \"Back-off pulling image \\\"kicbase/echo-server:1.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="default/hello-minikube1-68d8f56889-wsf5f" podUID="5dfaae61-dd8b-4785-8419-0d0d03ed9b0f"
Feb 26 12:41:22 minikube kubelet[1524]: E0226 12:41:22.564965    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"dashboard-metrics-scraper\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b-mqv47" podUID="067ad3f1-8946-4766-93a4-549d35de7555"
Feb 26 12:41:28 minikube kubelet[1524]: E0226 12:41:28.565029    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"angular-app\" with ImagePullBackOff: \"Back-off pulling image \\\"angular-app:latest\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="default/angular-app-66c6dfc877-p5jr9" podUID="685cb028-0c8d-43b9-a9e1-2077fa75a3f8"
Feb 26 12:41:29 minikube kubelet[1524]: E0226 12:41:29.568790    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b-jw9nx" podUID="3dc2447b-9a6f-4705-8a16-5a64aec02a1c"
Feb 26 12:41:31 minikube kubelet[1524]: E0226 12:41:31.563106    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"echo-server\" with ImagePullBackOff: \"Back-off pulling image \\\"kicbase/echo-server:1.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="default/hello-minikube1-68d8f56889-wsf5f" podUID="5dfaae61-dd8b-4785-8419-0d0d03ed9b0f"
Feb 26 12:41:32 minikube kubelet[1524]: E0226 12:41:32.564017    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"echo-server\" with ImagePullBackOff: \"Back-off pulling image \\\"kicbase/echo-server:1.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="default/hello-minikube-ffcbb5874-x8lsk" podUID="c71de88b-066d-4e3c-9760-af200fdf204d"
Feb 26 12:41:35 minikube kubelet[1524]: E0226 12:41:35.565065    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"dashboard-metrics-scraper\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b-mqv47" podUID="067ad3f1-8946-4766-93a4-549d35de7555"
Feb 26 12:41:39 minikube kubelet[1524]: E0226 12:41:39.570632    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"angular-app\" with ImagePullBackOff: \"Back-off pulling image \\\"angular-app:latest\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="default/angular-app-66c6dfc877-p5jr9" podUID="685cb028-0c8d-43b9-a9e1-2077fa75a3f8"
Feb 26 12:41:41 minikube kubelet[1524]: E0226 12:41:41.562916    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b-jw9nx" podUID="3dc2447b-9a6f-4705-8a16-5a64aec02a1c"
Feb 26 12:41:44 minikube kubelet[1524]: E0226 12:41:44.565987    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"echo-server\" with ImagePullBackOff: \"Back-off pulling image \\\"kicbase/echo-server:1.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="default/hello-minikube-ffcbb5874-x8lsk" podUID="c71de88b-066d-4e3c-9760-af200fdf204d"
Feb 26 12:41:46 minikube kubelet[1524]: E0226 12:41:46.562744    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"echo-server\" with ImagePullBackOff: \"Back-off pulling image \\\"kicbase/echo-server:1.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="default/hello-minikube1-68d8f56889-wsf5f" podUID="5dfaae61-dd8b-4785-8419-0d0d03ed9b0f"
Feb 26 12:41:46 minikube kubelet[1524]: E0226 12:41:46.562797    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"dashboard-metrics-scraper\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b-mqv47" podUID="067ad3f1-8946-4766-93a4-549d35de7555"
Feb 26 12:41:51 minikube kubelet[1524]: E0226 12:41:51.564187    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"angular-app\" with ImagePullBackOff: \"Back-off pulling image \\\"angular-app:latest\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="default/angular-app-66c6dfc877-p5jr9" podUID="685cb028-0c8d-43b9-a9e1-2077fa75a3f8"
Feb 26 12:41:52 minikube kubelet[1524]: E0226 12:41:52.564467    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b-jw9nx" podUID="3dc2447b-9a6f-4705-8a16-5a64aec02a1c"
Feb 26 12:41:56 minikube kubelet[1524]: E0226 12:41:56.564654    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"echo-server\" with ImagePullBackOff: \"Back-off pulling image \\\"kicbase/echo-server:1.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="default/hello-minikube-ffcbb5874-x8lsk" podUID="c71de88b-066d-4e3c-9760-af200fdf204d"
Feb 26 12:41:57 minikube kubelet[1524]: E0226 12:41:57.566405    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"dashboard-metrics-scraper\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b-mqv47" podUID="067ad3f1-8946-4766-93a4-549d35de7555"
Feb 26 12:42:01 minikube kubelet[1524]: E0226 12:42:01.562124    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"echo-server\" with ImagePullBackOff: \"Back-off pulling image \\\"kicbase/echo-server:1.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="default/hello-minikube1-68d8f56889-wsf5f" podUID="5dfaae61-dd8b-4785-8419-0d0d03ed9b0f"
Feb 26 12:42:03 minikube kubelet[1524]: E0226 12:42:03.565495    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"angular-app\" with ImagePullBackOff: \"Back-off pulling image \\\"angular-app:latest\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="default/angular-app-66c6dfc877-p5jr9" podUID="685cb028-0c8d-43b9-a9e1-2077fa75a3f8"
Feb 26 12:42:04 minikube kubelet[1524]: E0226 12:42:04.564313    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b-jw9nx" podUID="3dc2447b-9a6f-4705-8a16-5a64aec02a1c"
Feb 26 12:42:08 minikube kubelet[1524]: E0226 12:42:08.567801    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"dashboard-metrics-scraper\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b-mqv47" podUID="067ad3f1-8946-4766-93a4-549d35de7555"
Feb 26 12:42:09 minikube kubelet[1524]: E0226 12:42:09.572836    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"echo-server\" with ImagePullBackOff: \"Back-off pulling image \\\"kicbase/echo-server:1.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="default/hello-minikube-ffcbb5874-x8lsk" podUID="c71de88b-066d-4e3c-9760-af200fdf204d"
Feb 26 12:42:13 minikube kubelet[1524]: E0226 12:42:13.564263    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"echo-server\" with ImagePullBackOff: \"Back-off pulling image \\\"kicbase/echo-server:1.0\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="default/hello-minikube1-68d8f56889-wsf5f" podUID="5dfaae61-dd8b-4785-8419-0d0d03ed9b0f"
Feb 26 12:42:16 minikube kubelet[1524]: E0226 12:42:16.565657    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b-jw9nx" podUID="3dc2447b-9a6f-4705-8a16-5a64aec02a1c"
Feb 26 12:42:18 minikube kubelet[1524]: E0226 12:42:18.565906    1524 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"angular-app\" with ImagePullBackOff: \"Back-off pulling image \\\"angular-app:latest\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="default/angular-app-66c6dfc877-p5jr9" podUID="685cb028-0c8d-43b9-a9e1-2077fa75a3f8"


==> storage-provisioner [91ed578536ab] <==
I0226 11:53:44.635670       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0226 11:53:44.641425       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0226 11:53:44.641866       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0226 11:54:02.054684       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0226 11:54:02.054861       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"7810d3b6-9da8-496b-9d4a-2eae20186300", APIVersion:"v1", ResourceVersion:"9128", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_83f0f999-9db1-4960-b815-1be7501df053 became leader
I0226 11:54:02.054951       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_83f0f999-9db1-4960-b815-1be7501df053!
I0226 11:54:02.155718       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_83f0f999-9db1-4960-b815-1be7501df053!


==> storage-provisioner [b9315df8de0e] <==
I0226 11:53:21.263475       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0226 11:53:31.278192       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": net/http: TLS handshake timeout

